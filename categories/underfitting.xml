<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CODEBUG (Posts about underfitting)</title><link>https://sijanb.com.np/</link><description></description><atom:link href="https://sijanb.com.np/categories/underfitting.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:sijanonly@gmail.com"&gt;Sijan Bhandari&lt;/a&gt; </copyright><lastBuildDate>Sun, 02 Jun 2024 09:24:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>How to combat overfitting and underfitting in Machine Learning ?</title><link>https://sijanb.com.np/posts/how-to-combat-overfitting-and-underfitting-in-machine-learning/</link><dc:creator>Sijan Bhandari</dc:creator><description>&lt;p&gt;Machine Learning models learn the relationship between input (features) and output (target) using learnable parameters. The size of these parameters defines the complexity and flexibility of a given model.&lt;/p&gt;
&lt;p&gt;There are two typical scenarios. When the flexibility of a model is insufficient to capture the underlying pattern in a training dataset, the model is called underfitted. Conversely, when the model is too flexible to the underlying pattern, it is said that the model has “memorized” the training data, resulting in an overfitted model.&lt;/p&gt;
&lt;p&gt;Consider a system that can be explained by a quadratic function, but we use a simple line to represent it, i.e., a single parameter to capture the underlying trends in the data. Because the function lacks the required complexity to fit the data (two parameters), we end up with a poor predictor. In this case, the model will have high bias, meaning we will get consistent but consistently wrong answers. This is called an underfitted model.&lt;/p&gt;
&lt;p&gt;Now imagine that the true system is a parabola, but we use a higher-order polynomial to fit it. Due to natural noise in the data used to fit (deviations from the perfect parabola), the overly complex model treats these fluctuations and noise as intrinsic properties of the system and attempts to fit them. The result is a model with high variance.&lt;/p&gt;
&lt;p&gt;More details:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://sijanb.com.np/posts/what-is-overfitting-in-machine-learning/"&gt;Overfitting&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://sijanb.com.np/posts/what-is-underfitting-in-machine-learning/"&gt;Underfitting&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description><category>machine-learning</category><category>machine-learning-glossary</category><category>overfitting</category><category>underfitting</category><guid>https://sijanb.com.np/posts/how-to-combat-overfitting-and-underfitting-in-machine-learning/</guid><pubDate>Sun, 02 Jun 2024 09:19:06 GMT</pubDate></item><item><title>What is underfitting in Machine Learning ?</title><link>https://sijanb.com.np/posts/what-is-underfitting-in-machine-learning/</link><dc:creator>Sijan Bhandari</dc:creator><description>&lt;p&gt;Underfitting occurs in machine leanrning / data science when a data model fails to accurately capture the relationship between input and output variables, resulting in high error rates on both the training set and unseen data.
This also entails that the model has insufficient training duration or the input variables lack significance to establish a meaningful relationship between the input and output variables. As the model learns, its bias diminishes, but its variance may increase, leading to overfitting. The objective in model fitting is to identify the optimal balance between underfitting and overfitting (.i.e., finding the sweet spot), allowing the model to capture the dominant trend in the training data and generalize effectively to new datasets.&lt;/p&gt;
&lt;p&gt;Important details:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;High biased model (underfitted) is not able to learn the very basic/important patterns in the training data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adding more data and making your model simpler won't help to avoid underfitting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One should try other sophisticated models (e.g Decision tree in comparision to kNN) or add complexity in the current model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using complex models (example : polynomial regression rather than linear one) may be useful to capture the relevant patterns in the training data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adding more features (or derived features from existing one) will also increase the model capacity and helps to avoid underfitting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you see unacceptably high training error and test error, the model is underfitted.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;High bias and low variance are the indicators of underfitting models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Underfitting is easier to track than overfitting since the performance can be measured during training phase.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description><category>high-bias</category><category>machine-learning</category><category>machine-learning-glossary</category><category>underfitting</category><guid>https://sijanb.com.np/posts/what-is-underfitting-in-machine-learning/</guid><pubDate>Mon, 13 May 2024 19:40:08 GMT</pubDate></item></channel></rss>