<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CODEBUG (Posts about gradient-descent)</title><link>https://sijanb.com.np/</link><description></description><atom:link href="https://sijanb.com.np/categories/gradient-descent.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:sijanonly@gmail.com"&gt;Sijan Bhandari&lt;/a&gt; </copyright><lastBuildDate>Sat, 11 May 2024 08:22:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>What is gradient descent ?</title><link>https://sijanb.com.np/posts/what-is-gradient-descent/</link><dc:creator>Sijan Bhandari</dc:creator><description>&lt;p&gt;Gradient descent is an optimization algorithm which is commonly used to train machine learning models
and neural networks. Optimization refers to the task of minimizing/maximizing an objective function.
The process of optimization involves calculating the gradient (or the partial derivatives) of the cost
function with respect to each parameter (weights and biases) . To do so, the models are fed with training data iteratively.
and gradient points are calculated. The gradient points in the direction of the
steepest increase of the function. Gradient descent moves in the opposite direction—the
direction of the steepest descent—to reduce the cost function value.&lt;/p&gt;</description><category>gradient-descent</category><category>machine-learning</category><category>machine-learning-glossary</category><guid>https://sijanb.com.np/posts/what-is-gradient-descent/</guid><pubDate>Sat, 11 May 2024 07:02:15 GMT</pubDate></item></channel></rss>