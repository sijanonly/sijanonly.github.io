<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CODEBUG (Posts about gradient-descent)</title><link>https://sijanb.com.np/</link><description></description><atom:link href="https://sijanb.com.np/categories/gradient-descent.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2024 &lt;a href="mailto:sijanonly@gmail.com"&gt;Sijan Bhandari&lt;/a&gt; </copyright><lastBuildDate>Mon, 27 May 2024 17:59:28 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>What are different types of gradient descent algorithm in machine learning ?</title><link>https://sijanb.com.np/posts/what-are-different-types-of-gradient-descent-algorithm-in-machine-learning/</link><dc:creator>Sijan Bhandari</dc:creator><description>&lt;p&gt;There exist three distinct types of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.&lt;/p&gt;
&lt;p&gt;Batch Gradient Descent (BGD)&lt;/p&gt;
&lt;p&gt;In Batch Gradient Descent, the term 'batch' signifies the utilization of the entire training dataset during each iteration of the learning process. By incorporating all training examples for each update, Batch Gradient Descent ensures stable error gradients and a consistent trajectory towards the optimal solution, albeit with significant computational demands.
This batching method enhances computational efficiency; however, it can still result in extended processing times for large training datasets due to the necessity of storing all data in memory. While Batch Gradient Descent typically yields a stable error gradient and reliable convergence, it occasionally converges to a local minimum rather than the global optimum.&lt;/p&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD)&lt;/p&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD) enhances parameter updates by leveraging individual data points during each iteration. By conducting a training epoch for each dataset example and updating parameters sequentially, SGD minimizes memory requirements, as only a single training example needs to be stored at any given time. These frequent updates, while providing detailed and rapid adjustments, may lead to decreased computational efficiency relative to batch gradient descent. Despite the potential for noisy gradients, which arise from these frequent updates, this noise can facilitate the escape from local minima, thereby aiding in the pursuit of a global minimum. The principle of SGD is characterized by its utilization of a single example per iteration, hence the term "stochastic" reflects the random selection of each example within the batch. Given sufficient iterations, SGD proves effective, albeit with inherent noisiness.&lt;/p&gt;</description><category>gradient-descent</category><category>machine-learning</category><category>machine-learning-glossary</category><guid>https://sijanb.com.np/posts/what-are-different-types-of-gradient-descent-algorithm-in-machine-learning/</guid><pubDate>Mon, 27 May 2024 17:12:35 GMT</pubDate></item><item><title>What is gradient descent ?</title><link>https://sijanb.com.np/posts/what-is-gradient-descent/</link><dc:creator>Sijan Bhandari</dc:creator><description>&lt;p&gt;Gradient descent is a widely used optimization approach for training machine learning models and neural networks. Optimization is the process of minimizing or increasing an objective function.
Optimization entails calculating the gradient (partial derivatives) of the cost function for each parameter (weights and biases). To do this, the models are given training data iteratively.
And, the gradient points are determined. The gradient represents the steepest rise in the function. Gradient descent lowers cost function values by going in the opposite direction of the steepest decrease.&lt;/p&gt;</description><category>gradient-descent</category><category>machine-learning</category><category>machine-learning-glossary</category><guid>https://sijanb.com.np/posts/what-is-gradient-descent/</guid><pubDate>Sat, 11 May 2024 07:02:15 GMT</pubDate></item></channel></rss>