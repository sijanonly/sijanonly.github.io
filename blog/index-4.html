<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Data Exploration...">
<meta name="viewport" content="width=device-width">
<title>CODEBUG (page 4) | CODEBUG</title>
<link href="../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../assets/css/screen.css">
<link rel="stylesheet" type="text/css" href="../assets/css/nav.css">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata">
<link href="../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="https://sijanb.com.np/blog/index-4.html">
<link rel="icon" href="../favicon.ico" sizes="16x16">
<link rel="prev" href="." type="text/html">
<link rel="next" href="index-3.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>
<body class="nav-closed">

<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
<li class="nav-opened" role="presentation">
            <a href=".">Blog</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../categories/">Tags</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../rss.xml">RSS feed</a>
        </li>
    
    
    </ul>
</div>
<span class="nav-cover"></span>

<div class="site-wrapper">
    <header class="main-header post-head no-cover"><nav class="main-nav overlay clearfix"><a class="blog-logo" href="https://sijanb.com.np/"><img src="../images/logo.png" alt="Blog Logo"></a>
            <a class="menu-button" href="#"><span class="burger">☰</span><span class="word">Menu</span></a>
        </nav><div class="vertical">
            <div class="main-header-content inner">
                <h1 class="page-title">CODEBUG (page 4)</h1>
                <h2 class="page-description">Data Exploration...</h2>
            </div>
        </div>
        <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
    </header><main id="content" class="content" role="main"><div class="postindex">


<article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-overfitting-in-machine-learning/">What is overfitting in Machine Learning ?</a></h2>
    </header><section class="post-excerpt"><p>Overfitting occurs when the model attempts to match the training set too closely. On fresh data, the overfitted model is unable to produce accurate predictions.</p>
<p>Important details:</p>
<ol class="arabic simple">
<li><p>The model will attempt to match the data too closely and will pick up on noise in the data when the training data set is limited or the given model is complex.</p></li>
<li><p>An overfitted model picks up patterns that are unique to the training set and overlooks the generic patterns.</p></li>
<li><p>Regularization can reduce overfitting.</p></li>
<li><p>Overfitting can also be decreased by training on a large and diversed training data points.</p></li>
<li><p>Overfitting can be detected by high variation .i.e, if the test data has a high error rate while the training data has a low error rate.</p></li>
<li><p>A high variance model will overfit the data and is flexible in capturing every detail—relevant or not—and noise in the data.</p></li>
<li><p>A high variance model is also indicated as: Training error &lt;&lt; Validation error.</p></li>
<li><p>More training data will improve the generalization of the given model and avoids overfitting.</p></li>
</ol></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/high-variance/">#high-variance</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,
                <a href="../categories/overfitting/">#overfitting</a>,

        <time class="post-date" datetime="2024-05-12T17:38:25+05:45">
            2024-05-12
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-gradient-descent/">What is gradient descent ?</a></h2>
    </header><section class="post-excerpt"><p>Short:</p>
<p>Gradient descent is an optimization algorithm used to determine the coefficients / parameters of a function (f) that minimize a cost function.</p>
<p><a class="reference external" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/">reference</a></p>
<p>Detail:</p>
<p>Gradient descent is a widely used optimization approach for training machine learning models and neural networks. Optimization is the process of minimizing or increasing an objective function.
Optimization entails calculating the gradient (partial derivatives) of the cost function for each parameter (weights and biases). To do this, the models are given training data iteratively.
And, the gradient points are determined. The gradient consistently indicates the direction of the steepest increase in the loss function. The gradient descent algorithm proceeds by taking a step in the direction of the negative gradient to minimize the loss as efficiently as possible.</p>
<ul class="simple">
<li><p>The selection of the learning rate has a substantial influence on the effectiveness of gradient descent. An excessively high learning rate may cause the algorithm to overshoot the minimum, while an excessively low learning rate may result in prolonged convergence times.</p></li>
<li><p>Due to its non-convexity, the loss function L(w) of a neural network is generally known to potentially have multiple local minima. When multiple local minima are present, it is highly likely that the algorithm may fail to converge to a global minimum. Thus, local minima pose significant challenges as they may cause the training process to stall rather than progress towards the global minimum.</p></li>
<li><p>Gradient descent, despite being a widely utilized optimization algorithm, does not ensure convergence to an optimum in all scenarios. Various factors can hinder convergence: Saddle Points: In high-dimensional spaces, gradient descent may become trapped at saddle points where the gradient is zero but does not correspond to a minimum.</p></li>
<li><p>Saddle points in a multivariable function are critical points where the function does not achieve either a local maximum or a local minimum value.</p></li>
<li><p>A common issue with both local minima and saddle points is the presence of plateaus with low curvature in the error landscape. Although gradient descent dynamics are repelled from a saddle point towards lower error by following directions of negative curvature, this repulsion can be slow due to the plateau.</p></li>
<li><p>Stochastic Gradient Descent (SGD) can occasionally escape simple saddle points if fluctuations occur in different directions and the step size is sufficiently large to overcome the flatness. However, saddle regions can sometimes be quite complex.</p></li>
<li><p>The gradient of error, defined over the difference between the actual and predicted outputs, approaches zero at a local minimum, causing progress to stall due to weight correction steps being proportional to the gradient's magnitude, which is near zero at a minimum. Techniques such as 'random weight initiation' can be employed to avoid this issue.</p></li>
</ul></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/gradient-descent/">#gradient-descent</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-11T12:47:15+05:45">
            2024-05-11
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-inductive-bias-in-machine-learning/">What is Inductive Bias in Machine Learning ?</a></h2>
    </header><section class="post-excerpt"><p>An explicit or implicit assumption or prior information about the model that permits it to generalize
outside of the training set of data is known as inductive bias.</p>
<p>Examples of inductive bias:</p>
<ol class="arabic simple">
<li><p>When it comes to decision trees, shorter trees work better than longer ones.</p></li>
<li><p>The response variable (y) in linear regression is thought to vary linearly in predictors (X).</p></li>
<li><p>In general, the belief that the most simplest hypothesis is more accurate than the more complicated one (Occam's razor) .</p></li>
</ol></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/inductive-bias/">#inductive-bias</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-10T01:06:31+05:45">
            2024-05-10
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/machine-learning-glossary-what-are-model-training-steps/">What are model training steps in machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>There may exist many possible models to solve a given problem at hand. Based on your modeling decision there are usually two different ways to complete the machine learning lifecycle.</p>
<ul class="simple">
<li><p>1st scenario. Training a single model with a training dataset and final evaluation with the test set.</p></li>
<li><p>2nd scenario. Training multiple models with training/validation dataset and final evaluation with the test set.</p></li>
</ul>
<p>In case of (1st scenario), you will follow the following approach:</p>
<ul class="simple">
<li><p>Divide the data into training and test sets. (Usually 70/30 splits)</p></li>
<li><p>Select your preferable model.</p></li>
<li><p>Train it with a training dataset.</p></li>
<li><p>Assess the trained model in the test set. (no need to perform validation in your trained model)</p></li>
</ul>
<p>In case of (2nd scenario), you will follow the following approach:</p>
<ul class="simple">
<li><p>Divide the data into training, validation, and test sets. (Usually 50/25/25 splits)</p></li>
<li><p>Select the initial model/architecture.</p></li>
<li><p>Train the model with a training dataset.</p></li>
<li><p>Evaluate the model using the validation dataset.</p></li>
<li><p>Repeat steps (b) through (d) for different models or training parameters.</p></li>
<li><p>Select the best model based on evaluation and train the best model with combined (training + validation) datasets.</p></li>
<li><p>Assess the trained model in the test set.</p></li>
</ul></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,
                <a href="../categories/model-evaluation/">#model-evaluation</a>,
                <a href="../categories/training-validation-test/">#training-validation-test</a>,

        <time class="post-date" datetime="2024-05-07T00:36:31+05:45">
            2024-05-07
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/machine-learning-glossary-what-is-model-training-in-machine-learning/">what is model training in machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>The Machine Learning model is represented by the model parameters. Those parameters are the learnable parameters. Learning happens when these parameters are updated with suitable values and the model is able to solve the given tasks.
Training is the process of feeding a training dataset to your model. The training process uses an objective function (example MSE) to get the feedback in each iteration. Since we are trying to improve the accuracy of the model on a given
input, and lower the error between model prediction and actual output, we also called training process as a model optimization process.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-06T23:44:07+05:45">
            2024-05-06
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/machine-learning-glossary-what-is-machine-learning/">What is machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>Understanding and extracting hidden patterns or features from the data is the learning process in machine learning. Instead of using explicit
logic supplied by people, machine learning has the capacity to learn from experiences.
Conventional systems are created with the use of well defined human-set rules. In order for machine learning algorithms
to understand complicated patterns from inputs (x), they use outputs (y) as a feedback signal. Thus, an intelligent program is the ML system's
final product.</p>
<p>We often use a logical method to solve any issue. We make an effort to break the task up into several smaller tasks and solve each smaller task
using a distinct rationale. When dealing with extremely complicated jobs, like stock price prediction, the patterns are always changing,
which has an impact on the results.
That implies that, in order to answer this problem logically, we must adjust our handwritten logic for each new change in the outputs.
Machine Learning (ML), on the other hand, creates the model using a vast amount of data. The data gives the model all of its historical experience,
which helps it better understand the pattern. We just retrain the model with fresh instances whenever the data changes.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-05T16:13:45+05:45">
            2024-05-05
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/paper-summary-playing-atari-with-deep-reinforcement-learning/">Paper Summary : Playing Atari With Deep Reinforcement Learning</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">"Playing Atari with Deep Reinforcement Learning"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="../posts/paper-summary-playing-atari-with-deep-reinforcement-learning/#Motivation">¶</a>
</h5>
<p>Deep Learning (DL) has proven to work well when we have large amount of data. Unline supervised DL algorithm setup, Reinforcement Learning (RL) doesn't have direct access to the targets/labels. RL agent usually get "delayed and sparsed" rewards as a signal to understand about the environment and learn policy for a given environment.
Another challenge is about the distribution of the inputs. In supervised learning, each batch in training loop is drawn randomly which make sure each inputs/samples are independent and the parameter updates won't overfit to some specific direction/class in the data. In case of RL, inputs are usually correlated. For example, when you collect image inputs/frames of video of games, their pixel positions won't change much. Therefore, many samples will look alike and this might lead to poor learning and local optimal solution. Another problem is the non-stationarity of the target. The target will be changing throughout the episodes when the agent learns new behaviour from the environment, or adopting well.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="../posts/paper-summary-playing-atari-with-deep-reinforcement-learning/#Contribution">¶</a>
</h5>
<p>Authors proposed 'Deep Q Network' (DQN) learning algorithm with experience replay. This approach solves both the correlated inputs and non-stationarity problems.</p>
<p>They uses CNN with a variant of Q-learning algorithm, and uses stochastic gradient descent (SGD) for the training. They maintained a buffer named - 'Experience Replay' of the transitions while the agent nagivates through the environment. While SGD training process, samples from this stored buffer is used to create mini-batches and used for the training of the NN. This refer this NN as Q-network with parameter, $ \theta $, which minimizes the sequences of loss functions $ L_i (\theta_i) $ :</p>
<p>$ L_i(\theta_i) $ = $ \mathbb{E_{s,a \sim \rho(.)}} [ (y_i - Q(s, a; \theta_i)^2 ] $</p>
<p>Where $ y_i = \mathbb{E_{s' \sim  \varepsilon}} [ r + \gamma \underset{a'} max (s', a', \theta_{i-}) | s,a] $</p>
<p>is the target for iteration i.</p>
<p>They used the previous iteration parameter value ($ \theta_{i-1} $) in order to calculate the target ($y_i$). The parameter ($ \theta_{i-1} $) from previous iteration won't change for some long future iterations, which makes it stationary and training will be smooth. They also  feed concatenation of four video frames as an input to the CNN in order to avoid the partial observation contraints in the learning. Using four frames, CNN will be able locate the movement direction, speed of the objects in the frames.</p>
<p>DQN is used to train on Atari 2600 games. The video frames from emulator are the observations based on discrete actions (up, down, left, rigth..) of the agent in the environment. The network consists of two convolutional layers and two fully connected layers. The last layer outputs the distribution over possible actions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2020-11-08T19:25:42+05:45">
            2020-11-08
        </time></footer></article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="." rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-3.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blog-sijanb-com-np";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></main><footer class="site-footer clearfix"><section class="poweredby">Contents © 2025         <a href="mailto:sijanonly@gmail.com">Sijan Bhandari</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </section></footer>
</div>

    <script type="text/javascript" src="../assets/js/jquery.js"></script><script type="text/javascript" src="../assets/js/jquery.fitvids.js"></script><script type="text/javascript" src="../assets/js/index.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116715433-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116715433-2');
</script>
</body>
</html>
