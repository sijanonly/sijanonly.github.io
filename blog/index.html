<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Data Exploration...">
<meta name="viewport" content="width=device-width">
<title>CODEBUG (page 5) | CODEBUG</title>
<link href="../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../assets/css/screen.css">
<link rel="stylesheet" type="text/css" href="../assets/css/nav.css">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata">
<link href="../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="https://sijanb.com.np/blog/">
<link rel="icon" href="../favicon.ico" sizes="16x16">
<link rel="next" href="index-4.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>
<body class="nav-closed">

<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
<li class="nav-opened nav-current" role="presentation">
            <a href=".">Blog</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../categories/">Tags</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../rss.xml">RSS feed</a>
        </li>
    
    
    </ul>
</div>
<span class="nav-cover"></span>

<div class="site-wrapper">
    <header class="main-header post-head no-cover"><nav class="main-nav overlay clearfix"><a class="blog-logo" href="https://sijanb.com.np/"><img src="../images/logo.png" alt="Blog Logo"></a>
            <a class="menu-button" href="#"><span class="burger">☰</span><span class="word">Menu</span></a>
        </nav><div class="vertical">
            <div class="main-header-content inner">
                <h1 class="page-title">CODEBUG (page 5)</h1>
                <h2 class="page-description">Data Exploration...</h2>
            </div>
        </div>
        <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
    </header><main id="content" class="content" role="main"><div class="postindex">


<article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/how-foundation-model-is-changing-the-natural-language-processing-nlp-landscape/">How Foundation Model is changing the Natural Language Processing (NLP) landscape ?</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered" id="cell-id=d3118957">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img alt="No description has been provided for this image" src="../images/foundation_models_landscape.png">
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=a3548a81">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Foundation Models in Natural Language Processing: A Comprehensive Overview</strong></p>
<p><strong>The Language Revolution in AI</strong></p>
<p>Language is fundamental to human communication, influencing our thoughts, relationships, and knowledge acquisition. Every society develops complex spoken or signed languages, which children learn effortlessly. This complexity poses a significant challenge in artificial intelligence research.</p>
<p>Natural Language Processing (NLP) focuses on enabling computers to understand and generate human language. A transformative shift occurred in 2018 with the advent of foundation models, revolutionizing our approach to language technology.</p>
<p><strong>The Foundation Model Revolution</strong></p>
<p><strong>Key Features of Foundation Models</strong></p>
<p>Foundation models mark a significant departure from traditional methods that relied on specialized tools for specific tasks. Instead of developing separate systems for translation, summarization, or sentiment analysis, we now utilize adaptable models for multiple purposes.</p>
<p>For instance, GPT-3 functions like a Swiss Army knife, capable of writing stories, answering questions, translating languages, and generating code from a single framework.</p>
<p><strong>Traditional vs. Modern Approaches</strong></p>
<p><em>Traditional Approach (Pre-2018):</em></p>
<ul>
<li>Separate teams for each task (translation, parsing, classification)</li>
<li>Complex systems with multiple specialized models</li>
<li>Extensive engineering for task-specific architectures</li>
</ul>
<p><em>Foundation Model Approach (Post-2018):</em></p>
<ul>
<li>A single versatile model for various tasks</li>
<li>Simple training objective: "predict the next word"</li>
<li>Minimal customization for specific tasks</li>
</ul>
<p><strong>Significant Advancements:</strong> In 2018, the top system for 8th-grade science questions achieved a score of 73.1%. By 2019, an adapted foundation model improved this to 91.6%, highlighting the effectiveness of this new paradigm.</p>
<p><strong>Transformative Impact of Foundation Models</strong></p>
<p><strong>From Understanding to Generation</strong></p>
<p>Prior to foundation models, coherent text generation was deemed nearly impossible. Researchers primarily focused on text analysis. The breakthrough came with the realization that training models to predict the next word could yield coherent and functional text generation.</p>
<p><strong>Practical Applications:</strong> Modern AI can now produce product descriptions, marketing content, draft emails, and create stories that closely resemble human writing.</p>
<p><strong>Universal Language Proficiency</strong></p>
<p>Foundation models exhibit extraordinary versatility. A single model can:</p>
<ul>
<li>Classify sentiment in movie reviews</li>
<li>Extract names and organizations from documents</li>
<li>Translate languages</li>
<li>Summarize lengthy articles</li>
<li>Engage in conversations</li>
</ul>
<p>This adaptability stems from their training on extensive text datasets, enabling them to learn general language patterns applicable across diverse contexts.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=d5182463">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>The Global Language Challenge: Embracing Multilingualism</strong></p>
<p>While foundation models excel in English, over 6,000 languages are spoken worldwide, with only a few having sufficient digital text for dedicated AI training. For instance, Fula, a West African language with 65 million speakers, lacks the necessary digital resources for robust AI development.</p>
<p><strong>Multilingual Solutions</strong></p>
<p>Current multilingual models, such as mBERT and XLM-R, tackle this issue by:</p>
<ul>
<li>Training on around 100 languages simultaneously</li>
<li>Utilizing common linguistic structures</li>
<li>Transferring insights from high-resource languages to low-resource ones</li>
</ul>
<p>However, challenges persist, including:</p>
<ul>
<li>Abundant, higher-quality English data</li>
<li>Better performance of languages similar to English</li>
<li>Competition among languages for model capacity</li>
</ul>
<p><strong>Learning from Human Language Acquisition</strong></p>
<p>Humans acquire language efficiently; children achieve linguistic competence with limited exposure, while models like GPT-3 require significantly more data. Key differences include:</p>
<p><strong>Human Learning:</strong></p>
<ul>
<li>Grounded in real-world experiences</li>
<li>Learns generalizable patterns</li>
<li>Adapts continuously</li>
<li>Understands meaning before mastering syntax</li>
</ul>
<p><strong>AI Learning:</strong></p>
<ul>
<li>Based on statistical text patterns without real-world context</li>
<li>Inconsistent application of learned patterns</li>
<li>Static post-training</li>
<li>May learn syntax before grasping meaning</li>
</ul>
<p>For example, a child learns "dog" through experience, while an AI model learns from text, lacking true understanding.</p>
<p><strong>Current Limitations and Future Directions</strong></p>
<p><strong>Systematicity Challenge</strong></p>
<p>Foundation models often lack the systematic understanding present in human language use, leading to inconsistent application of grammatical structures.</p>
<p><strong>Language Variation and Equity</strong></p>
<p>Challenges include:</p>
<ul>
<li>Dialectal variations</li>
<li>Differences between informal and formal registers</li>
<li>Cultural and social linguistic disparities</li>
<li>Representation of minority languages</li>
</ul>
<p>Foundation models risk reinforcing linguistic inequalities by favoring dominant language varieties.</p>
<p><strong>Practical Applications</strong></p>
<p>Foundation models currently enhance:</p>
<ul>
<li>Content creation for blogs and marketing</li>
<li>Customer service via chatbots</li>
<li>Translation for improved communication</li>
<li>Educational tools for personalized learning</li>
<li>Code generation for programming assistance</li>
</ul>
<p><strong>Research Transformation</strong></p>
<p>The focus has shifted from developing task-specific architectures to optimizing the use of foundation models, enhancing adaptation methods, and analyzing model behavior.</p>
<p><strong>Looking Ahead: The Future of Language AI</strong></p>
<p><strong>Emerging Research Directions</strong></p>
<ul>
<li>Grounded Language Learning: Linking language to real-world contexts</li>
<li>Improved Multilingual Coverage: Better representation of global languages</li>
<li>Systematic Generalization: Achieving human-like consistency</li>
<li>Adaptive Learning: Models that evolve over time</li>
<li>Fairness and Equity: Ensuring performance equality across language varieties</li>
</ul>
<p><strong>Challenges Ahead</strong></p>
<p>Despite progress, gaps remain in model performance versus real-world deployment needs. Ongoing efforts aim to:</p>
<ul>
<li>Reduce computational demands</li>
<li>Enhance reliability and consistency</li>
<li>Address bias and fairness</li>
<li>Boost multilingual capabilities</li>
</ul>
<p><strong>Conclusion</strong></p>
<p>Foundation models have transformed natural language processing into a unified, adaptable approach. While challenges in efficiency, multilingual coverage, and systematic understanding persist, these models have reached human-level performance in many complex language tasks. The future of language AI hinges on bridging the gap between current capabilities and human-level understanding, ensuring equitable service across all languages and communities. This evolution in NLP marks the beginning of a new era in AI's ability to engage with the rich complexity of human language.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=6593db78">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>citation: Bommasani, Rishi, et al. "On the Opportunities and Risks of Foundation Models." arXiv preprint arXiv:2108.07258 (2021)
arXiv: <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a></p>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/bert/">#BERT</a>,
                <a href="../categories/foundation-models/">#foundation-models</a>,
                <a href="../categories/large-language-models/">#large-language-models</a>,
                <a href="../categories/llms/">#llms</a>,
                <a href="../categories/openai/">#openai</a>,
                <a href="../categories/transformers/">#transformers</a>,

        <time class="post-date" datetime="2025-06-12T01:03:53+05:45">
            2025-06-12
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-the-future-of-foundation-models/">What is the future of Foundation Models ?</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered" id="cell-id=09ec12c8">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Future-of-AI-Foundation-Models:-Who-Will-Shape-Tomorrow's-Technology?">The Future of AI Foundation Models: Who Will Shape Tomorrow's Technology?<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Future-of-AI-Foundation-Models:-Who-Will-Shape-Tomorrow's-Technology?">¶</a>
</h2>
<h3 id="The-Early-Days-of-AI:-Understanding-Our-Current-Landscape">The Early Days of AI: Understanding Our Current Landscape<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Early-Days-of-AI:-Understanding-Our-Current-Landscape">¶</a>
</h3>
<p>While recent advancements like ChatGPT have made headlines, we are still in the initial phase of the foundation model revolution. Picture it like the internet in 1995—we recognize the immense potential, yet we are still navigating the necessary rules, standards, and best practices.</p>
<p>At this moment, these advanced AI systems function as "research prototypes" available to the public. It's akin to taking experimental vehicles for a spin on public roads—thrilling but accompanied by uncertain risks and outcomes.</p>
<h3 id="A-Crucial-Inquiry:-Who-Will-Guide-AI's-Future?">A Crucial Inquiry: Who Will Guide AI's Future?<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#A-Crucial-Inquiry:-Who-Will-Guide-AI's-Future?">¶</a>
</h3>
<p>The evolution of foundation models prompts a vital question that will influence technological advancements for the next decade: <strong>Who will steer the development of AI?</strong> The answer will impact various facets of society, from job markets to democratic processes.</p>
<h3 id="The-Divide:-Industry-vs.-Academia">The Divide: Industry vs. Academia<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Divide:-Industry-vs.-Academia">¶</a>
</h3>
<h4 id="The-Current-Landscape:-Industry's-Dominance">The Current Landscape: Industry's Dominance<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Current-Landscape:-Industry's-Dominance">¶</a>
</h4>
<p>Presently, the AI sector is primarily influenced by a select group of powerful players:</p>
<p><strong>Major Tech Companies:</strong></p>
<ul>
<li>
<strong>Google</strong>: Innovators behind PaLM and Bard</li>
<li>
<strong>Microsoft/OpenAI</strong>: Creators of the GPT series and ChatGPT</li>
<li>
<strong>Meta (Facebook)</strong>: Developers of LLaMA models</li>
<li>
<strong>Amazon</strong>: Expanding various AI services</li>
</ul>
<p><strong>AI Startups:</strong></p>
<ul>
<li>
<strong>OpenAI</strong>: Groundbreaking yet increasingly commercial</li>
<li>
<strong>Anthropic</strong>: Dedicated to AI safety</li>
<li>
<strong>AI21 Labs</strong>: Focused on specialized language models</li>
</ul>
<p><strong>Reasons for Industry Leadership:</strong></p>
<ul>
<li>
<strong>Extensive computational resources</strong>: Training costs can reach millions.</li>
<li>
<strong>Access to user data</strong>: Billions of interactions enhance model performance.</li>
<li>
<strong>Expert engineering teams</strong>: Specialists dedicated to full-time work.</li>
<li>
<strong>Rapid development</strong>: Ability to move swiftly without academic constraints.</li>
</ul>
<h4 id="The-Importance-of-Academia:-Why-Universities-Matter">The Importance of Academia: Why Universities Matter<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Importance-of-Academia:-Why-Universities-Matter">¶</a>
</h4>
<p>Universities offer something that the industry often overlooks: <strong>a variety of perspectives and a commitment to the public good</strong>.</p>
<p><strong>Academic Strengths:</strong></p>
<ul>
<li>
<strong>Interdisciplinary collaboration</strong>: Computer scientists partner with ethicists, sociologists, and economists.</li>
<li>
<strong>Long-term focus</strong>: Not motivated by short-term profits.</li>
<li>
<strong>Commitment to public interest</strong>: Striving for societal benefit.</li>
<li>
<strong>Diverse student representation</strong>: Reflecting a wide range of backgrounds and viewpoints.</li>
</ul>
<p><strong>Example</strong>: While a tech firm may aim to make its chatbot more engaging for increased user retention, a university team might investigate how to develop AI assistants that foster healthy digital habits.</p>
<h3 id="Balancing-Incentives:-Market-Forces-vs.-Social-Good">Balancing Incentives: Market Forces vs. Social Good<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#Balancing-Incentives:-Market-Forces-vs.-Social-Good">¶</a>
</h3>
<h4 id="When-Profit-Aligns-with-Public-Benefit">When Profit Aligns with Public Benefit<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#When-Profit-Aligns-with-Public-Benefit">¶</a>
</h4>
<p>At times, commercial interests can lead to beneficial outcomes for everyone:</p>
<p><strong>Positive Scenarios:</strong></p>
<ul>
<li>
<strong>Enhanced accuracy</strong>: Companies desire dependable AI, leading to better performance for users.</li>
<li>
<strong>User-friendly designs</strong>: Intuitive AI tools promote wider adoption and accessibility.</li>
<li>
<strong>Efficiency gains</strong>: Faster, cost-effective AI benefits both businesses and consumers.</li>
</ul>
<h4 id="The-Pitfalls-of-Profit-Driven-Development">The Pitfalls of Profit-Driven Development<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Pitfalls-of-Profit-Driven-Development">¶</a>
</h4>
<p>However, profit-centric approaches can create significant blind spots:</p>
<p><strong>The "Malaria Problem":</strong>
Similar to how pharmaceutical companies may neglect malaria treatments due to low profitability in affected regions, tech companies might overlook AI applications that aid marginalized communities lacking financial return.</p>
<p><strong>Real-World Consequences:</strong></p>
<ul>
<li>
<strong>Digital divide</strong>: Advanced AI tools may only be accessible to affluent users.</li>
<li>
<strong>Language biases</strong>: AI focuses on profitable languages, sidelining smaller communities.</li>
<li>
<strong>Accessibility issues</strong>: Limited development for users with disabilities.</li>
<li>
<strong>Environmental neglect</strong>: Energy-intensive training practices may be disregarded if costs are externalized.</li>
</ul>
<p><strong>Example</strong>: An AI firm might invest millions in perfecting a shopping assistant for luxury goods while neglecting tools to support social workers aiding homeless populations.</p>
<h3 id="The-Accessibility-Challenge:-The-Shift-Away-from-Openness">The Accessibility Challenge: The Shift Away from Openness<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Accessibility-Challenge:-The-Shift-Away-from-Openness">¶</a>
</h3>
<h4 id="The-Open-AI-Research-Era-(2010s)">The Open AI Research Era (2010s)<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Open-AI-Research-Era-(2010s)">¶</a>
</h4>
<p>There was a time when AI research was becoming more open and collaborative:</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>
<strong>Open-source frameworks</strong>: Tools like TensorFlow and PyTorch made AI more accessible.</li>
<li>
<strong>Shared datasets</strong>: Facilitated collaborative building on previous work.</li>
<li>
<strong>Reproducible research</strong>: Enabled verification and enhancement of studies.</li>
<li>
<strong>Community standards</strong>: Conferences required reproducibility protocols.</li>
</ul>
<p><strong>Outcome</strong>: This openness fostered rapid innovation, allowing researchers globally to collaborate and compete fairly.</p>
<h4 id="The-Current-Closed-Landscape:-Challenges-with-Foundation-Models">The Current Closed Landscape: Challenges with Foundation Models<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Current-Closed-Landscape:-Challenges-with-Foundation-Models">¶</a>
</h4>
<p>Foundation models have reversed this trend towards openness:</p>
<p><strong>Access Limitations:</strong></p>
<ul>
<li>
<strong>API-only access</strong>: Models like GPT-3 are only available through controlled interfaces.</li>
<li>
<strong>Lack of model releases</strong>: Complete systems are frequently kept confidential.</li>
<li>
<strong>Opaque datasets</strong>: Training data is often not shared with researchers.</li>
<li>
<strong>High computational demands</strong>: Training now necessitates corporate-level budgets.</li>
</ul>
<p><strong>Illustrative Example</strong>: Imagine if only car manufacturers were allowed to test new safety features, while independent safety researchers were barred from accessing crash test data. This scenario mirrors the current state of AI research.</p>
<h3 id="The-Scale-Factor:-Understanding-the-Importance-of-Size">The Scale Factor: Understanding the Importance of Size<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Scale-Factor:-Understanding-the-Importance-of-Size">¶</a>
</h3>
<h4 id="The-Emergence-of-Capabilities">The Emergence of Capabilities<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-Emergence-of-Capabilities">¶</a>
</h4>
<p>Certain AI capabilities only manifest at large scales:</p>
<p><strong>In-Context Learning Example:</strong></p>
<ul>
<li>
<strong>Smaller models</strong>: Unable to learn new tasks from contextual examples during interactions.</li>
<li>
<strong>Larger models</strong>: Gain this ability once they reach a sufficient size.</li>
<li>
<strong>Research Implication</strong>: Studying certain phenomena requires billion-dollar models.</li>
</ul>
<p><strong>Analogy</strong>: Understanding weather patterns is akin to studying hurricanes—you can't grasp the full picture by only observing small clouds; you need satellite-level data.</p>
<h4 id="Academic-Strategies-and-Their-Limitations">Academic Strategies and Their Limitations<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#Academic-Strategies-and-Their-Limitations">¶</a>
</h4>
<p><strong>Current Approaches:</strong></p>
<ul>
<li>
<strong>Research on smaller models</strong>: Attempting to extrapolate insights to larger scales.</li>
<li>
<strong>Analysis of existing models</strong>: Focusing on released models rather than training new ones.</li>
<li>
<strong>Collaborative efforts</strong>: Initiatives like EleutherAI pooling resources together.</li>
</ul>
<p><strong>Challenges of These Strategies</strong>:
Attempting to comprehend ocean currents by only examining puddles, or researching automotive safety using only bicycles, illustrates the inadequacy of these methods. Certain phenomena can only be understood at scale.</p>
<h3 id="The-BERT-Dilemma:-Over-Reliance-on-a-Single-Model">The BERT Dilemma: Over-Reliance on a Single Model<a class="anchor-link" href="../posts/what-is-the-future-of-foundation-models/#The-BERT-Dilemma:-Over-Reliance-on-a-Single-Model">¶</a>
</h3>
<p>A significant amount of contemporary AI research is founded on BERT, a specific Google model, leading to several challenges:</p>
<p><strong>Risks of Dependency:</strong></p>
<ul>
<li>
<strong>Arbitrary design choices</strong>: BERT's specific architecture influences all subsequent research.</li>
<li>
<strong>Ingrained biases</strong>: All derivative work inherits BERT's limitations.</li>
<li>
<strong>Innovation restrictions</strong>: Researchers may optimize for BERT's structure instead of exploring alternatives.</li>
</ul>
<p><strong>Example</strong>: This situation is comparable to modern architecture relying solely on modifications of a single building design, rather than exploring a range of fundamentally different approaches.</p>
<p><strong>Exploring Potential Solutions: Creating an Equitable Landscape</strong></p>
<p><strong>Government Investment: Embracing the "Big Science" Model</strong></p>
<p><strong>Historical Precedents:</strong></p>
<ul>
<li>
<strong>Hubble Space Telescope:</strong> Paved the way for groundbreaking discoveries unattainable with smaller instruments.</li>
<li>
<strong>Large Hadron Collider:</strong> Demonstrated the necessity of global collaboration and substantial funding.</li>
<li>
<strong>Human Genome Project:</strong> Showcased the benefits of public investment in essential research.</li>
</ul>
<p><strong>AI Counterpart:</strong>
Establishing a National Research Cloud could equip academic researchers with the computational power necessary for training and analyzing large foundational models.</p>
<p><strong>Envisioning This Initiative:</strong></p>
<ul>
<li>
<strong>Shared Computing Infrastructure:</strong> Universities would gain access to high-performance computing resources.</li>
<li>
<strong>Open Datasets:</strong> Publicly funded and ethically curated training data available for all.</li>
<li>
<strong>Collaborative Platforms:</strong> A space for researchers globally to contribute and share knowledge.</li>
</ul>
<p><strong>Volunteer Computing: Making AI Development Accessible</strong></p>
<p><strong>Successful Initiatives:</strong></p>
<ul>
<li>
<strong>Folding@home:</strong> Engaged millions of home computers to simulate protein folding.</li>
<li>
<strong>SETI@home:</strong> Conducted a distributed search for extraterrestrial intelligence.</li>
<li>
<strong>Bitcoin Mining:</strong> Illustrated the potential of distributed computing on a massive scale.</li>
</ul>
<p><strong>AI Application:</strong>
The Learning@home project seeks to harness volunteer computing power from personal devices around the world to train foundational models.</p>
<p><strong>Challenges to Consider:</strong></p>
<ul>
<li>
<strong>Network Limitations:</strong> Home internet connections may not have the capacity for extensive data processing.</li>
<li>
<strong>Coordination Complexity:</strong> Synchronizing training efforts across millions of devices can be intricate.</li>
<li>
<strong>Security Concerns:</strong> Safeguarding volunteer nodes from potential breaches is crucial.</li>
</ul>
<p><strong>The Importance: Why This Matters for Everyone</strong></p>
<p><strong>Scenario 1: A Future Dominated by Industry</strong>
If current trends persist, we may see:</p>
<ul>
<li>
<strong>Concentration of Power:</strong> A limited number of companies controlling AI development.</li>
<li>
<strong>Commercial Bias:</strong> AI systems designed with profit as a priority over public good.</li>
<li>
<strong>Restricted Innovation:</strong> A lack of diverse approaches and perspectives.</li>
<li>
<strong>Democratic Concerns:</strong> Essential technologies managed by unaccountable entities.</li>
</ul>
<p><strong>Scenario 2: A Future with Academic Balance</strong>
With adequate investment in academic research, we could achieve:</p>
<ul>
<li>
<strong>Diverse Development:</strong> Multiple methodologies and philosophies in AI design.</li>
<li>
<strong>Public Interest Focus:</strong> AI created for the benefit of society at large.</li>
<li>
<strong>Open Innovation:</strong> Shared tools and knowledge fostering rapid advancement.</li>
<li>
<strong>Democratic Oversight:</strong> Public institutions playing a role in guiding AI development.</li>
</ul>
<p><strong>Real-World Consequences</strong></p>
<p><strong>Education:</strong>
Current risk: AI tutoring systems tailored for engagement rather than deep learning.
Alternative: AI designed to foster thorough understanding and critical thinking skills.</p>
<p><strong>Healthcare:</strong>
Current risk: AI diagnostic tools accessible only to affluent hospitals.
Alternative: Open-source AI tools available to community health centers.</p>
<p><strong>Employment:</strong>
Current risk: AI deployment focused on reducing costs at the expense of worker well-being.
Alternative: AI systems aimed at enhancing human capabilities rather than replacing them.</p>
<p><strong>The Path Ahead: Collaboration Over Competition</strong></p>
<p><strong>What Industry Offers:</strong></p>
<ul>
<li>
<strong>Resources and Scale:</strong> The capacity to train and implement large models.</li>
<li>
<strong>Engineering Expertise:</strong> Skills for managing complex systems.</li>
<li>
<strong>Real-World Deployment:</strong> Experience with applications for consumers.</li>
<li>
<strong>Speed and Agility:</strong> Rapid development and refinement.</li>
</ul>
<p><strong>What Academia Contributes:</strong></p>
<ul>
<li>
<strong>Ethical Perspective:</strong> Insight into social implications.</li>
<li>
<strong>Disciplinary Diversity:</strong> Varied viewpoints on intricate issues.</li>
<li>
<strong>Long-Term Vision:</strong> Research extending beyond immediate commercial interests.</li>
<li>
<strong>Public Accountability:</strong> Commitment to transparent, peer-reviewed studies.</li>
</ul>
<p><strong>The Ideal Collaboration</strong>
The future of foundational models should be characterized by:</p>
<ul>
<li>
<strong>Shared Infrastructure:</strong> Government-funded computing resources for research, and open datasets through public-private partnerships.</li>
<li>
<strong>Ethical Integration:</strong> Involvement of social scientists and ethicists from the onset, along with public feedback on AI design.</li>
<li>
<strong>Balanced Innovation:</strong> Merging industry efficiency with academic integrity, ensuring commercial applications align with public interest.</li>
</ul>
<p><strong>Conclusion: A Crucial Moment</strong>
We find ourselves at a pivotal point in AI development. The choices made in the coming years will shape whether foundational models serve all of humanity or cater primarily to a select few.</p>
<p>The stakes are immense. These models will significantly influence:</p>
<ul>
<li>Our methods of working and learning.</li>
<li>Our access to information and decision-making processes.</li>
<li>The functionality of democratic institutions.</li>
<li>The distribution of economic opportunities.</li>
</ul>
<p>Achieving success requires acknowledging that this challenge extends beyond technical aspects—it's a social, ethical, and political issue that necessitates a diverse array of human expertise and perspectives.</p>
<p>The future of AI should not be dictated solely by those with the most resources. Instead, it should be crafted by a variety of voices collaborating to ensure these powerful technologies benefit the common good.</p>
<p>The pressing question is not whether foundational models will reshape society, but rather if that transformation will be advantageous for everyone or just a privileged few. The outcome hinges on the decisions we make today.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=ce7e2bf2">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>citation: Bommasani, Rishi, et al. "On the Opportunities and Risks of Foundation Models." arXiv preprint arXiv:2108.07258 (2021)
arXiv: <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a></p>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/bert/">#BERT</a>,
                <a href="../categories/foundation-models/">#foundation-models</a>,
                <a href="../categories/large-language-models/">#large-language-models</a>,
                <a href="../categories/llms/">#llms</a>,
                <a href="../categories/openai/">#openai</a>,
                <a href="../categories/transformers/">#transformers</a>,

        <time class="post-date" datetime="2025-06-10T13:02:08+05:45">
            2025-06-10
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/">How foundational Models differs from other models such as machine learning or deep learning models ?</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered" id="cell-id=8b934665">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img alt="No description has been provided for this image" src="../images/foundation_models_difference.png">
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=d28cc5f0">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exploring-Foundation-Models:-A-Guide-to-the-AI-Revolution">Exploring Foundation Models: A Guide to the AI Revolution<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Exploring-Foundation-Models:-A-Guide-to-the-AI-Revolution">¶</a>
</h2>
<h3 id="The-Two-Forces-Shaping-AI:-Emergence-and-Homogenization">The Two Forces Shaping AI: Emergence and Homogenization<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#The-Two-Forces-Shaping-AI:-Emergence-and-Homogenization">¶</a>
</h3>
<p>Artificial Intelligence has experienced a significant transformation over the last thirty years, driven by two key forces that are redefining the development and implementation of AI systems.</p>
<p><strong>Emergence</strong> refers to the natural development of capabilities during training, rather than through explicit programming. Imagine teaching a child to ride a bike; you don't dictate every movement, but through practice, the skill emerges organically.</p>
<p><strong>Homogenization</strong> indicates the adoption of similar methodologies across various challenges. Instead of creating entirely distinct solutions for each problem, we now implement standardized techniques that are applicable across multiple scenarios.</p>
<h3 id="The-Three-Stages-of-AI-Evolution">The Three Stages of AI Evolution<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#The-Three-Stages-of-AI-Evolution">¶</a>
</h3>
<h4 id="Stage-1:-The-Machine-Learning-Revolution-(1990s)">Stage 1: The Machine Learning Revolution (1990s)<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Stage-1:-The-Machine-Learning-Revolution-(1990s)">¶</a>
</h4>
<p>Prior to machine learning, programmers had to define specific rules for every task. For instance, to identify spam emails, one might code rules like "if the email includes 'FREE MONEY', classify as spam."</p>
<p>Machine learning revolutionized this process by allowing computers to learn from examples. Instead of coding rules, we provided computers with thousands of emails labeled as "spam" or "not spam," enabling them to autonomously recognize patterns.</p>
<p><strong>Example</strong>: Instead of crafting specific rules for credit approvals, banks could supply historical loan data to algorithms that learn to predict loan defaults based on data patterns.</p>
<h4 id="Stage-2:-The-Deep-Learning-Breakthrough-(2010s)">Stage 2: The Deep Learning Breakthrough (2010s)<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Stage-2:-The-Deep-Learning-Breakthrough-(2010s)">¶</a>
</h4>
<p>Traditional machine learning required human experts to identify crucial features. For image recognition, specialists had to manually determine that edges, corners, and textures were important to detect.</p>
<p>Deep learning changed this dynamic by enabling models to automatically identify significant features. Neural networks with multiple layers could learn complex patterns directly from unprocessed data.</p>
<p><strong>Example</strong>: Rather than programming a computer to recognize cats by identifying pointy ears and whiskers, deep learning systems could automatically learn these characteristics by analyzing thousands of cat images.</p>
<h4 id="Stage-3:-The-Foundation-Models-Era-(2018-Present)">Stage 3: The Foundation Models Era (2018-Present)<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Stage-3:-The-Foundation-Models-Era-(2018-Present)">¶</a>
</h4>
<p>Foundation models mark the latest advancement, where a single large model is trained on extensive datasets and then tailored for various tasks.</p>
<p><strong>Key Innovation</strong>: These models utilize "self-supervised learning," allowing them to learn by predicting missing data parts instead of relying on human-labeled examples.</p>
<p><strong>Example</strong>: A language model like GPT learns by processing billions of web pages and predicting the next word in sentences. Through this method, it acquires an understanding of grammar, facts, reasoning, and even creative writing—without direct instruction.</p>
<h3 id="Real-World-Applications-and-Examples">Real-World Applications and Examples<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Real-World-Applications-and-Examples">¶</a>
</h3>
<h4 id="Healthcare">Healthcare<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Healthcare">¶</a>
</h4>
<p>A single foundation model trained on medical images, patient records, and research documents could be utilized to:</p>
<ul>
<li>Diagnose skin conditions from images</li>
<li>Analyze X-rays for fractures</li>
<li>Predict patient outcomes</li>
<li>Aid in drug discovery</li>
</ul>
<h4 id="Education">Education<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Education">¶</a>
</h4>
<p>The same foundational model could enhance:</p>
<ul>
<li>Personalized tutoring systems</li>
<li>Automated essay evaluation</li>
<li>Language learning applications</li>
<li>Generation of educational content</li>
</ul>
<h4 id="Business-Operations">Business Operations<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Business-Operations">¶</a>
</h4>
<p>Foundation models facilitate:</p>
<ul>
<li>Customer service chatbots</li>
<li>Document analysis and summarization</li>
<li>Code generation and debugging</li>
<li>Market analysis and forecasting</li>
</ul>
<h3 id="The-Significance-of-Emergent-Capabilities">The Significance of Emergent Capabilities<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#The-Significance-of-Emergent-Capabilities">¶</a>
</h3>
<p>One of the most intriguing aspects of foundation models is their emergent abilities—skills that arise unexpectedly as models grow larger.</p>
<p><strong>In-Context Learning Example</strong>: GPT-3 showcased the ability to learn new tasks from conversation examples without additional training. By providing a few examples of translating English to French, it can continue translating, despite not being specifically trained for this task.</p>
<p>This is akin to a student who, after extensively studying literature, suddenly excels at writing poetry without ever having taken a poetry class.</p>
<h3 id="The-Double-Edged-Sword:-Advantages-and-Challenges">The Double-Edged Sword: Advantages and Challenges<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#The-Double-Edged-Sword:-Advantages-and-Challenges">¶</a>
</h3>
<h4 id="Benefits-of-Homogenization">Benefits of Homogenization<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Benefits-of-Homogenization">¶</a>
</h4>
<ul>
<li>
<strong>Efficiency</strong>: Enhancements to one foundation model benefit all applications built upon it.</li>
<li>
<strong>Accessibility</strong>: Smaller organizations gain access to powerful AI without needing to develop from the ground up.</li>
<li>
<strong>Cross-domain Innovation</strong>: Successful techniques in one area can swiftly transfer to others.</li>
</ul>
<h4 id="Risks-of-Homogenization">Risks of Homogenization<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Risks-of-Homogenization">¶</a>
</h4>
<ul>
<li>
<strong>Single Points of Failure</strong>: Flaws in widely-used foundation models can propagate across all applications.</li>
<li>
<strong>Bias Amplification</strong>: Prejudices in training data can be inherited by all applications.</li>
<li>
<strong>Lack of Diversity</strong>: Over-reliance on similar methods may stifle innovation.</li>
</ul>
<h3 id="Looking-Ahead:-The-Multimodal-Future">Looking Ahead: The Multimodal Future<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Looking-Ahead:-The-Multimodal-Future">¶</a>
</h3>
<p>The next frontier involves creating models that can process various types of data simultaneously.</p>
<p><strong>Example</strong>: A future foundation model could:</p>
<ul>
<li>Read a news article about a scientific discovery</li>
<li>Analyze related research papers and data charts</li>
<li>Watch video explanations of the concept</li>
<li>Generate a comprehensive summary with visual aids</li>
</ul>
<p>This represents the pinnacle of homogenization—one model capable of handling text, images, audio, and video together.</p>
<h3 id="Key-Takeaways-for-the-Future">Key Takeaways for the Future<a class="anchor-link" href="../posts/how-foundational-models-differs-from-other-models-such-as-machine-learning-or-deep-learning-models/#Key-Takeaways-for-the-Future">¶</a>
</h3>
<ol>
<li>
<p><strong>Foundation models are becoming the backbone of modern AI</strong>, similar to how operating systems serve as the foundation for computer applications.</p>
</li>
<li>
<p><strong>Emergence presents both opportunities and challenges</strong>—we gain powerful capabilities that we didn't explicitly program, but we also encounter unexpected behaviors.</p>
</li>
<li>
<p><strong>The concentration of AI development around a few foundation models</strong> yields efficiency gains but also presents systemic risks that require careful management.</p>
</li>
<li>
<p><strong>Grasping these models remains a vital challenge</strong>—their complexity complicates the prediction of their behavior and the identification of potential issues.</p>
</li>
</ol>
<p>As we progress, the primary challenge lies in leveraging the immense potential of foundation models while managing the inherent risks associated with their widespread use. This calls for ongoing research aimed at enhancing the interpretability, reliability, and alignment of these systems with human values.</p>
<p>citation: Bommasani, Rishi, et al. "On the Opportunities and Risks of Foundation Models." arXiv preprint arXiv:2108.07258 (2021)
arXiv: <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a></p>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2025-06-08T21:11:25+05:45">
            2025-06-08
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-are-large-language-models-llms/">What are Large Language Models (LLMs) ?</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered" id="cell-id=d8aa7401">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>The Evolution of Large Language Models: From Turing's Vision to the Reality of ChatGPT</strong></p>
<p><strong>A 70-Year Journey: From the Turing Test to Contemporary AI</strong></p>
<p>The journey to develop machines that genuinely grasp human language began in the 1950s with Alan Turing’s introduction of his renowned test for machine intelligence. This monumental challenge posed the question: how can we teach computers to understand the intricacies and nuances of human language?</p>
<p>Language transcends mere words; it is a complex system enriched with grammar rules, cultural contexts, implied meanings, and creative expression. Imagine attempting to convey sarcasm, poetry, or humor to someone unfamiliar with human emotions. This was the challenge engineers confronted while designing machines capable of understanding language.</p>
<p><strong>Three Stages of Evolution in Language AI</strong></p>
<p><strong>Stage 1: Statistical Language Models (1990s-2010s)</strong><br>
Early language models functioned like advanced autocomplete systems, relying on statistical patterns to predict subsequent words. For instance, if you entered "The weather is," the system would analyze millions of examples to suggest words like "nice," "cold," or "sunny" based on observed frequency patterns.<br><strong>Limitations:</strong> While these models could complete sentences, they lacked true comprehension of meaning or context beyond a few words.</p>
<p><strong>Stage 2: Neural Language Models (2010s)</strong><br>
The advent of neural networks transformed language processing, allowing models to grasp context and word relationships. For example, unlike statistical models, neural networks could discern that "bank" has different meanings in "river bank" and "savings bank" by evaluating the surrounding context.<br><strong>Breakthrough:</strong> Models like BERT (2018) significantly improved language comprehension by enabling them to read entire sentences and understand the interconnections between words.</p>
<p><strong>Stage 3: Large Language Models - The Current Revolution (2020s-Present)</strong><br>
A remarkable breakthrough emerged when researchers discovered that enlarging language models significantly enhanced their performance and granted them new capabilities.</p>
<p><strong>The Importance of Scale: Discovering the Impact of Size</strong><br>
Researchers identified that when language models exceeded specific size thresholds—transitioning from millions to hundreds of billions of parameters—extraordinary advancements occurred. These models not only excelled in existing tasks but also developed entirely new abilities.<br>
Consider it this way: imagine learning to play the piano, and upon reaching a certain level, you suddenly find yourself able to compose symphonies without formal training in composition.</p>
<p><strong>What Defines a "Large" Language Model?</strong><br>
Modern Large Language Models are characterized by:</p>
<ul>
<li>Hundreds of billions of parameters, in contrast to older models with millions.</li>
<li>Training on extensive text datasets sourced from the internet.</li>
<li>Transformer architecture that enables the processing and understanding of relationships between words over lengthy passages.</li>
</ul>
<p>For instance, GPT-3 boasts 175 billion parameters—imagine a brain with 175 billion adjustable connections, each fine-tuned through exposure to a vast array of human-written knowledge.</p>
<p><strong>Emergent Abilities: Unforeseen Capabilities</strong><br>
One of the most astonishing features is "in-context learning," which allows models to acquire new tasks simply by observing examples within a conversation.<br><strong>Example:</strong></p>
<p>If you present the model with: "Dog -&gt; Animal, Rose -&gt; Flower, Oak -&gt; ?”<br>
It can respond with: "Tree"<br>
This demonstrates its ability to recognize patterns (specific items to their categories) from the examples provided.</p>
<p><strong>Additional Emergent Abilities:</strong></p>
<ul>
<li>
<strong>Complex reasoning:</strong> Solving intricate multi-step math problems.</li>
<li>
<strong>Creative writing:</strong> Producing poetry, stories, and scripts.</li>
<li>
<strong>Code generation:</strong> Writing functional computer programs.</li>
<li>
<strong>Language translation:</strong> Converting text between languages even if not specifically trained for those translations.</li>
</ul>
<p>Summary:</p>
<p>Language models create and produce text by predicting the likelihood of a word or series of words appearing within a larger
context. This capability is particularly beneficial for tasks such as text generation and translation.</p>
<p>Large language models (LLMs) are sophisticated models that utilize extensive parameters and large datasets,
allowing them to handle longer text sequences and execute complex functions like summarization and answering questions.</p>
<p>Transformers serve as a fundamental architecture in LLMs, employing attention mechanisms to prioritize significant parts
of the input, which improves processing efficiency.</p>
<p>LLMs offer a wide range of applications, including text generation, translation, sentiment analysis, and code generation.
However, they also raise important considerations regarding costs, biases, and ethical implications.</p>
<p>Citation: Zhao, Wayne Xin, et al. "A Survey of Large Language Models." arXiv preprint arXiv:2303.18223 (2023).
arXiv: <a href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a></p>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/large-language-models/">#large-language-models</a>,
                <a href="../categories/llms/">#llms</a>,
                <a href="../categories/transformers/">#transformers</a>,

        <time class="post-date" datetime="2025-06-08T19:46:53+05:45">
            2025-06-08
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/understanding-high-bias-in-machine-learning-with-real-world-example/">Understanding High Bias in Machine Learning with Real-World Example</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered" id="cell-id=40aa6e5c">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>High bias in machine learning results in underfitting, characterized by the model making oversimplified assumptions about the relationships within the data. This leads to subpar performance on both training and test datasets, demonstrating that the model does not possess the necessary complexity to capture the underlying patterns.</p>
<p>In this example, we will check house price prediction using two methods:</p>
<ol>
<li>Linear Regression: The simple linear model does not adequately capture the complex relationships between features and house prices.</li>
<li>Polynomial Regression: The PolynomialFeatures step enhances the dataset by generating polynomial and interaction terms from the original features. For instance, when you have a feature x with a degree of 2, it will produce new features such as x, x², and cross-terms like x1 * x2. This approach enables a linear regression model to effectively capture non-linear relationships.</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=3cfe78df">
<div class="input">
<div class="prompt input_prompt">In [31]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_scorer</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=b700af4d">
<div class="input">
<div class="prompt input_prompt">In [21]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">house_price_dataset</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fetch_california_housing</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=54eaaab7">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=a952b9bf">
<div class="input">
<div class="prompt input_prompt">In [22]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">house_price_dataset</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=9751a745">
<div class="input">
<div class="prompt input_prompt">In [23]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Loading the dataset to a pandas dataframe</span>
<span class="n">df_house_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">house_price_dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">house_price_dataset</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=db6e95bf">
<div class="input">
<div class="prompt input_prompt">In [24]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df_house_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[24]:</div>
<div class="output_html rendered_html output_subarea output_execute_result"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead><tr style="text-align: right;">
<th></th>
<th>MedInc</th>
<th>HouseAge</th>
<th>AveRooms</th>
<th>AveBedrms</th>
<th>Population</th>
<th>AveOccup</th>
<th>Latitude</th>
<th>Longitude</th>
</tr></thead>
<tbody>
<tr>
<th>0</th>
<td>8.3252</td>
<td>41.0</td>
<td>6.984127</td>
<td>1.023810</td>
<td>322.0</td>
<td>2.555556</td>
<td>37.88</td>
<td>-122.23</td>
</tr>
<tr>
<th>1</th>
<td>8.3014</td>
<td>21.0</td>
<td>6.238137</td>
<td>0.971880</td>
<td>2401.0</td>
<td>2.109842</td>
<td>37.86</td>
<td>-122.22</td>
</tr>
<tr>
<th>2</th>
<td>7.2574</td>
<td>52.0</td>
<td>8.288136</td>
<td>1.073446</td>
<td>496.0</td>
<td>2.802260</td>
<td>37.85</td>
<td>-122.24</td>
</tr>
<tr>
<th>3</th>
<td>5.6431</td>
<td>52.0</td>
<td>5.817352</td>
<td>1.073059</td>
<td>558.0</td>
<td>2.547945</td>
<td>37.85</td>
<td>-122.25</td>
</tr>
<tr>
<th>4</th>
<td>3.8462</td>
<td>52.0</td>
<td>6.281853</td>
<td>1.081081</td>
<td>565.0</td>
<td>2.181467</td>
<td>37.85</td>
<td>-122.25</td>
</tr>
</tbody>
</table>
</div></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=ac8cfa52">
<div class="input">
<div class="prompt input_prompt">In [25]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># add the target column</span>
<span class="c1"># target is median house value in block group (in $100,000s).</span>
<span class="n">df_house_data</span><span class="p">[</span><span class="s1">'price'</span><span class="p">]</span> <span class="o">=</span> <span class="n">house_price_dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=060b5817">
<div class="input">
<div class="prompt input_prompt">In [26]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df_house_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[26]:</div>
<div class="output_html rendered_html output_subarea output_execute_result"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead><tr style="text-align: right;">
<th></th>
<th>MedInc</th>
<th>HouseAge</th>
<th>AveRooms</th>
<th>AveBedrms</th>
<th>Population</th>
<th>AveOccup</th>
<th>Latitude</th>
<th>Longitude</th>
<th>price</th>
</tr></thead>
<tbody>
<tr>
<th>0</th>
<td>8.3252</td>
<td>41.0</td>
<td>6.984127</td>
<td>1.023810</td>
<td>322.0</td>
<td>2.555556</td>
<td>37.88</td>
<td>-122.23</td>
<td>4.526</td>
</tr>
<tr>
<th>1</th>
<td>8.3014</td>
<td>21.0</td>
<td>6.238137</td>
<td>0.971880</td>
<td>2401.0</td>
<td>2.109842</td>
<td>37.86</td>
<td>-122.22</td>
<td>3.585</td>
</tr>
<tr>
<th>2</th>
<td>7.2574</td>
<td>52.0</td>
<td>8.288136</td>
<td>1.073446</td>
<td>496.0</td>
<td>2.802260</td>
<td>37.85</td>
<td>-122.24</td>
<td>3.521</td>
</tr>
<tr>
<th>3</th>
<td>5.6431</td>
<td>52.0</td>
<td>5.817352</td>
<td>1.073059</td>
<td>558.0</td>
<td>2.547945</td>
<td>37.85</td>
<td>-122.25</td>
<td>3.413</td>
</tr>
<tr>
<th>4</th>
<td>3.8462</td>
<td>52.0</td>
<td>6.281853</td>
<td>1.081081</td>
<td>565.0</td>
<td>2.181467</td>
<td>37.85</td>
<td>-122.25</td>
<td>3.422</td>
</tr>
</tbody>
</table>
</div></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=15132a2f">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="House-Price-Prediction-Using-Linear-Regression">House Price Prediction Using Linear Regression<a class="anchor-link" href="../posts/understanding-high-bias-in-machine-learning-with-real-world-example/#House-Price-Prediction-Using-Linear-Regression">¶</a>
</h5>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=86ef8746">
<div class="input">
<div class="prompt input_prompt">In [34]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Prepare data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_house_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'price'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_house_data</span><span class="p">[</span><span class="s1">'price'</span><span class="p">]</span>

<span class="c1"># Split and scale</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Train linear model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate</span>
<span class="n">train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Training MSE:"</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Test MSE:"</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">))</span>

<span class="n">train_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training R² Score: </span><span class="si">{</span><span class="n">train_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test R² Score: </span><span class="si">{</span><span class="n">test_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Training MSE: 0.5240457125963887
Test MSE: 0.5261093658365182
Training R² Score: 0.6081
Test R² Score: 0.5980
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=aa6f2c1f">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="House-Price-Prediction-Using-Polynomial-Regression">House Price Prediction Using Polynomial Regression<a class="anchor-link" href="../posts/understanding-high-bias-in-machine-learning-with-real-world-example/#House-Price-Prediction-Using-Polynomial-Regression">¶</a>
</h5>
<ul>
<li>Observations:<ul>
<li>
<p>The mean squared error is decreased in test set.</p>
</li>
<li>
<p>The R² score is increased in test set.</p>
<p>-- R² Interpretation: An R-squared value of 0.75 indicates that 75% of the variation in house prices can be attributed to factors such as square footage, location, and the amenities included in the model.</p>
</li>
</ul>
</li>
</ul>
<p>Why Use both <code>PolynomialFeatures</code> and <code>LinearRegression</code> in the Pipeline:</p>
<pre><code>- The first PolynomialFeatures transformation creates a more complex feature space.
- The LinearRegression then fits a linear model to these non-linear features.
- This effectively allows a linear model to approximate non-linear relationships.</code></pre>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=b5e52638">
<div class="input">
<div class="prompt input_prompt">In [35]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">polynomial_regression_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># Create pipeline</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">'scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">'poly'</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">'linear'</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">])</span>
   
    <span class="c1"># Fit and evaluate</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Evaluate</span>
    <span class="n">train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
    <span class="n">test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Training MSE:"</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_pred</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Test MSE:"</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">))</span>

    <span class="n">train_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training R² Score: </span><span class="si">{</span><span class="n">train_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test R² Score: </span><span class="si">{</span><span class="n">test_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">X_train_poly</span><span class="p">,</span> <span class="n">X_test_poly</span>

<span class="c1"># Example usage with housing data</span>
<span class="n">model_poly</span><span class="p">,</span> <span class="n">X_train_poly</span><span class="p">,</span> <span class="n">X_test_poly</span> <span class="o">=</span> <span class="n">polynomial_regression_model</span><span class="p">(</span>
    <span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">X_test_scaled</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Training MSE: 0.4219834148836872
Test MSE: 0.42363567392919027
Training R² Score: 0.6844
Test R² Score: 0.6763
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=011b7ec6">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/high-bias/">#high-bias</a>,
                <a href="../categories/underfitting/">#underfitting</a>,

        <time class="post-date" datetime="2025-01-25T20:31:28+05:45">
            2025-01-25
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/how-to-combat-overfitting-and-underfitting-in-machine-learning/">How to combat overfitting and underfitting in Machine Learning ?</a></h2>
    </header><section class="post-excerpt"><p>Machine Learning models learn the relationship between input (features) and output (target) using learnable parameters. The size of these parameters defines the complexity and flexibility of a given model.</p>
<p>There are two typical scenarios. When the flexibility of a model is insufficient to capture the underlying pattern in a training dataset, the model is called underfitted. Conversely, when the model is too flexible to the underlying pattern, it is said that the model has “memorized” the training data, resulting in an overfitted model.</p>
<p>Consider a system that can be explained by a quadratic function, but we use a simple line to represent it, i.e., a single parameter to capture the underlying trends in the data. Because the function lacks the required complexity to fit the data (two parameters), we end up with a poor predictor. In this case, the model will have high bias, meaning we will get consistent but consistently wrong answers. This is called an underfitted model.</p>
<p>Now imagine that the true system is a parabola, but we use a higher-order polynomial to fit it. Due to natural noise in the data used to fit (deviations from the perfect parabola), the overly complex model treats these fluctuations and noise as intrinsic properties of the system and attempts to fit them. The result is a model with high variance.</p>
<p>More details:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://sijanb.com.np/posts/what-is-overfitting-in-machine-learning/">Overfitting</a></p></li>
<li><p><a class="reference external" href="https://sijanb.com.np/posts/what-is-underfitting-in-machine-learning/">Underfitting</a></p></li>
</ul></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,
                <a href="../categories/overfitting/">#overfitting</a>,
                <a href="../categories/underfitting/">#underfitting</a>,

        <time class="post-date" datetime="2024-06-02T15:04:06+05:45">
            2024-06-02
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-the-trade-off-between-bias-and-variance-in-machine-learning/">What is the trade-off between bias and variance in machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>Short:
A model with minimal parameters may exhibit high bias and low variance, while a model with numerous parameters may demonstrate high variance and low bias. Therefore, it is essential to achieve an optimal balance to avoid overfitting and underfitting the data. High bias arises from incorrect assumptions made by the learning algorithm, whereas variance arises from a model's sensitivity to minor variations in the training dataset.</p>
<p>Detail:
During development, all algorithms exhibit some degree of bias and variance. Models can be adjusted to address either bias or variance, but it is impossible to reduce both to zero without adversely affecting the other. This introduces the concept of the bias-variance trade-off.
Bias refers to the discrepancy between the average prediction of our model and the actual value being predicted, indicating the presence of systematic errors in the model.
Every algorithm inherently possesses some level of bias due to assumptions made within the model to simplify learning the target function. High bias can lead to underfitting, where the algorithm fails to capture relevant relationships between features and target outputs. Simpler algorithms tend to introduce more bias, whereas nonlinear algorithms usually have lower bias. These errors can originate from various sources, including the selection of training data, feature choices, or the training algorithm itself.
Variance measures how much a model's predictions change with different training sets, indicating the degree of over-specialization to a particular training set (overfitting). The goal is to assess the deviation of our model from the best possible model for the training data.
The ideal model seeks to minimize both bias and variance, achieving a balance that is neither too simple nor too complex, thereby yielding minimal error.
Low-variance models typically have a simple structure and are less sophisticated, but they risk being highly biased. Examples include Regression and Naive Bayes.
Conversely, low-bias models generally have a more flexible and complex structure but are prone to high variance. Examples include Nearest Neighbors and Decision Trees. Overfitting arises when a model is overly complex and learns the noise in the data rather than the actual signals.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/bias-variance-tradeoff/">#bias-variance-tradeoff</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-29T00:36:40+05:45">
            2024-05-29
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-are-different-types-of-gradient-descent-algorithm-in-machine-learning/">What are different types of gradient descent algorithm in machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>There exist three distinct types of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.</p>
<p>Batch Gradient Descent (BGD)</p>
<p>In Batch Gradient Descent, the term 'batch' signifies the utilization of the entire training dataset during each iteration of the learning process. By incorporating all training examples for each update, Batch Gradient Descent ensures stable error gradients and a consistent trajectory towards the optimal solution, albeit with significant computational demands.
This batching method enhances computational efficiency; however, it can still result in extended processing times for large training datasets due to the necessity of storing all data in memory. While Batch Gradient Descent typically yields a stable error gradient and reliable convergence, it occasionally converges to a local minimum rather than the global optimum.</p>
<p>Stochastic Gradient Descent (SGD)</p>
<p>Stochastic Gradient Descent (SGD) enhances parameter updates by leveraging individual data points during each iteration. By conducting a training epoch for each dataset example and updating parameters sequentially, SGD minimizes memory requirements, as only a single training example needs to be stored at any given time. These frequent updates, while providing detailed and rapid adjustments, may lead to decreased computational efficiency relative to batch gradient descent. Despite the potential for noisy gradients, which arise from these frequent updates, this noise can facilitate the escape from local minima, thereby aiding in the pursuit of a global minimum. The principle of SGD is characterized by its utilization of a single example per iteration, hence the term "stochastic" reflects the random selection of each example within the batch. Given sufficient iterations, SGD proves effective, albeit with inherent noisiness.</p>
<p>Mini Batch Gradient Descent</p>
<p>Mini-batch gradient descent integrates principles from both batch gradient descent and stochastic gradient descent. It partitions the training dataset into smaller batch sizes and executes updates on each of these batches. This methodology achieves a balance between the computational efficiency of batch gradient descent and the rapidity of stochastic gradient descent. Similar to stochastic gradient descent, the average cost over epochs in mini-batch gradient descent exhibits fluctuations due to the averaging of a limited number of examples at a time.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/gradient-descent/">#gradient-descent</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-27T22:57:35+05:45">
            2024-05-27
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-underfitting-in-machine-learning/">What is underfitting in Machine Learning ?</a></h2>
    </header><section class="post-excerpt"><p>Underfitting occurs in machine leanrning / data science when a data model fails to accurately capture the relationship between input and output variables, resulting in high error rates on both the training set and unseen data.
This also entails that the model has insufficient training duration or the input variables lack significance to establish a meaningful relationship between the input and output variables. As the model learns, its bias diminishes, but its variance may increase, leading to overfitting. The objective in model fitting is to identify the optimal balance between underfitting and overfitting (.i.e., finding the sweet spot), allowing the model to capture the dominant trend in the training data and generalize effectively to new datasets.</p>
<p>Important details:</p>
<ol class="arabic simple">
<li><p>High biased model (underfitted) is not able to learn the very basic/important patterns in the training data.</p></li>
<li><p>Adding more data and making your model simpler won't help to avoid underfitting.</p></li>
<li><p>One should try other sophisticated models (e.g Decision tree in comparision to kNN) or add complexity in the current model.</p></li>
<li><p>Using complex models (example : polynomial regression rather than linear one) may be useful to capture the relevant patterns in the training data.</p></li>
<li><p>Adding more features (or derived features from existing one) will also increase the model capacity and helps to avoid underfitting.</p></li>
<li><p>If you see unacceptably high training error and test error, the model is underfitted.</p></li>
<li><p>High bias and low variance are the indicators of underfitting models.</p></li>
<li><p>Underfitting is easier to track than overfitting since the performance can be measured during training phase.</p></li>
</ol></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/high-bias/">#high-bias</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,
                <a href="../categories/underfitting/">#underfitting</a>,

        <time class="post-date" datetime="2024-05-14T01:25:08+05:45">
            2024-05-14
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-overfitting-in-machine-learning/">What is overfitting in Machine Learning ?</a></h2>
    </header><section class="post-excerpt"><p>Overfitting occurs when the model attempts to match the training set too closely. On fresh data, the overfitted model is unable to produce accurate predictions.</p>
<p>Important details:</p>
<ol class="arabic simple">
<li><p>The model will attempt to match the data too closely and will pick up on noise in the data when the training data set is limited or the given model is complex.</p></li>
<li><p>An overfitted model picks up patterns that are unique to the training set and overlooks the generic patterns.</p></li>
<li><p>Regularization can reduce overfitting.</p></li>
<li><p>Overfitting can also be decreased by training on a large and diversed training data points.</p></li>
<li><p>Overfitting can be detected by high variation .i.e, if the test data has a high error rate while the training data has a low error rate.</p></li>
<li><p>A high variance model will overfit the data and is flexible in capturing every detail—relevant or not—and noise in the data.</p></li>
<li><p>A high variance model is also indicated as: Training error &lt;&lt; Validation error.</p></li>
<li><p>More training data will improve the generalization of the given model and avoids overfitting.</p></li>
</ol></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/high-variance/">#high-variance</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,
                <a href="../categories/overfitting/">#overfitting</a>,

        <time class="post-date" datetime="2024-05-12T17:38:25+05:45">
            2024-05-12
        </time></footer></article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-4.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blog-sijanb-com-np";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></main><footer class="site-footer clearfix"><section class="poweredby">Contents © 2025         <a href="mailto:sijanonly@gmail.com">Sijan Bhandari</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </section></footer>
</div>

    <script type="text/javascript" src="../assets/js/jquery.js"></script><script type="text/javascript" src="../assets/js/jquery.fitvids.js"></script><script type="text/javascript" src="../assets/js/index.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116715433-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116715433-2');
</script>
</body>
</html>
