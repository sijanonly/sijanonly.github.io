<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Data Exploration...">
<meta name="viewport" content="width=device-width">
<title>CODEBUG (page 4) | CODEBUG</title>
<link href="../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../assets/css/screen.css">
<link rel="stylesheet" type="text/css" href="../assets/css/nav.css">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata">
<link href="../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="https://sijanb.com.np/blog/">
<link rel="icon" href="../favicon.ico" sizes="16x16">
<link rel="next" href="index-3.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
</head>
<body class="nav-closed">

<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
<li class="nav-opened nav-current" role="presentation">
            <a href=".">Blog</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../categories/">Tags</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../rss.xml">RSS feed</a>
        </li>
    
    
    </ul>
</div>
<span class="nav-cover"></span>

<div class="site-wrapper">
    <header class="main-header post-head no-cover"><nav class="main-nav overlay clearfix"><a class="blog-logo" href="https://sijanb.com.np/"><img src="../images/logo.png" alt="Blog Logo"></a>
            <a class="menu-button" href="#"><span class="burger">☰</span><span class="word">Menu</span></a>
        </nav><div class="vertical">
            <div class="main-header-content inner">
                <h1 class="page-title">CODEBUG (page 4)</h1>
                <h2 class="page-description">Data Exploration...</h2>
            </div>
        </div>
        <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
    </header><main id="content" class="content" role="main"><div class="postindex">


<article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-gradient-descent/">What is gradient descent ?</a></h2>
    </header><section class="post-excerpt"><p>Gradient descent is a widely used optimization approach for training machine learning models and neural networks. Optimization is the process of minimizing or increasing an objective function.
Optimization entails calculating the gradient (partial derivatives) of the cost function for each parameter (weights and biases). To do this, the models are given training data iteratively.
And, the gradient points are determined. The gradient represents the steepest rise in the function. Gradient descent lowers cost function values by going in the opposite direction of the steepest decrease.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/gradient-descent/">#gradient-descent</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-11T12:47:15+05:45">
            2024-05-11
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/what-is-inductive-bias-in-machine-learning/">What is Inductive Bias in Machine Learning ?</a></h2>
    </header><section class="post-excerpt"><p>An explicit or implicit assumption or prior information about the model that permits it to generalize
outside of the training set of data is known as inductive bias.</p>
<p>Examples of inductive bias:</p>
<ol class="arabic simple">
<li><p>When it comes to decision trees, shorter trees work better than longer ones.</p></li>
<li><p>The response variable (y) in linear regression is thought to vary linearly in predictors (X).</p></li>
<li><p>In general, the belief that the most simplest hypothesis is more accurate than the more complicated one (Occam's razor) .</p></li>
</ol></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/inductive-bias/">#inductive-bias</a>,
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-10T01:06:31+05:45">
            2024-05-10
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/machine-learning-glossary-what-are-model-training-steps/">What are model training steps in machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>There may exist many possible models to solve a given problem at hand. Based on your modeling decision there are usually two different ways to complete the machine learning lifecycle.</p>
<ul class="simple">
<li><p>1st scenario. Training a single model with a training dataset and final evaluation with the test set.</p></li>
<li><p>2nd scenario. Training multiple models with training/validation dataset and final evaluation with the test set.</p></li>
</ul>
<p>In case of (1st scenario), you will follow the following approach:</p>
<ul class="simple">
<li><p>Divide the data into training and test sets. (Usually 70/30 splits)</p></li>
<li><p>Select your preferable model.</p></li>
<li><p>Train it with a training dataset.</p></li>
<li><p>Assess the trained model in the test set. (no need to perform validation in your trained model)</p></li>
</ul>
<p>In case of (2nd scenario), you will follow the following approach:</p>
<ul class="simple">
<li><p>Divide the data into training, validation, and test sets. (Usually 50/25/25 splits)</p></li>
<li><p>Select the initial model/architecture.</p></li>
<li><p>Train the model with a training dataset.</p></li>
<li><p>Evaluate the model using the validation dataset.</p></li>
<li><p>Repeat steps (b) through (d) for different models or training parameters.</p></li>
<li><p>Select the best model based on evaluation and train the best model with combined (training + validation) datasets.</p></li>
<li><p>Assess the trained model in the test set.</p></li>
</ul></section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,
                <a href="../categories/model-evaluation/">#model-evaluation</a>,
                <a href="../categories/training-validation-test/">#training-validation-test</a>,

        <time class="post-date" datetime="2024-05-07T00:36:31+05:45">
            2024-05-07
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/machine-learning-glossary-what-is-model-training-in-machine-learning/">what is model training in machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>The Machine Learning model is represented by the model parameters. Those parameters are the learnable parameters. Learning happens when these parameters are updated with suitable values and the model is able to solve the given tasks.
Training is the process of feeding a training dataset to your model. The training process uses an objective function (example MSE) to get the feedback in each iteration. Since we are trying to improve the accuracy of the model on a given
input, and lower the error between model prediction and actual output, we also called training process as a model optimization process.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-06T23:44:07+05:45">
            2024-05-06
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/machine-learning-glossary-what-is-machine-learning/">What is machine learning ?</a></h2>
    </header><section class="post-excerpt"><p>Understanding and extracting hidden patterns or features from the data is the learning process in machine learning. Instead of using explicit
logic supplied by people, machine learning has the capacity to learn from experiences.
Conventional systems are created with the use of well defined human-set rules. In order for machine learning algorithms
to understand complicated patterns from inputs (x), they use outputs (y) as a feedback signal. Thus, an intelligent program is the ML system's
final product.</p>
<p>We often use a logical method to solve any issue. We make an effort to break the task up into several smaller tasks and solve each smaller task
using a distinct rationale. When dealing with extremely complicated jobs, like stock price prediction, the patterns are always changing,
which has an impact on the results.
That implies that, in order to answer this problem logically, we must adjust our handwritten logic for each new change in the outputs.
Machine Learning (ML), on the other hand, creates the model using a vast amount of data. The data gives the model all of its historical experience,
which helps it better understand the pattern. We just retrain the model with fresh instances whenever the data changes.</p>
    </section><footer class="post-meta">
                Sijan Bhandari

        on
                <a href="../categories/machine-learning/">#machine-learning</a>,
                <a href="../categories/machine-learning-glossary/">#machine-learning-glossary</a>,

        <time class="post-date" datetime="2024-05-05T16:13:45+05:45">
            2024-05-05
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/paper-summary-playing-atari-with-deep-reinforcement-learning/">Paper Summary : Playing Atari With Deep Reinforcement Learning</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">"Playing Atari with Deep Reinforcement Learning"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="../posts/paper-summary-playing-atari-with-deep-reinforcement-learning/#Motivation">¶</a>
</h5>
<p>Deep Learning (DL) has proven to work well when we have large amount of data. Unline supervised DL algorithm setup, Reinforcement Learning (RL) doesn't have direct access to the targets/labels. RL agent usually get "delayed and sparsed" rewards as a signal to understand about the environment and learn policy for a given environment.
Another challenge is about the distribution of the inputs. In supervised learning, each batch in training loop is drawn randomly which make sure each inputs/samples are independent and the parameter updates won't overfit to some specific direction/class in the data. In case of RL, inputs are usually correlated. For example, when you collect image inputs/frames of video of games, their pixel positions won't change much. Therefore, many samples will look alike and this might lead to poor learning and local optimal solution. Another problem is the non-stationarity of the target. The target will be changing throughout the episodes when the agent learns new behaviour from the environment, or adopting well.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="../posts/paper-summary-playing-atari-with-deep-reinforcement-learning/#Contribution">¶</a>
</h5>
<p>Authors proposed 'Deep Q Network' (DQN) learning algorithm with experience replay. This approach solves both the correlated inputs and non-stationarity problems.</p>
<p>They uses CNN with a variant of Q-learning algorithm, and uses stochastic gradient descent (SGD) for the training. They maintained a buffer named - 'Experience Replay' of the transitions while the agent nagivates through the environment. While SGD training process, samples from this stored buffer is used to create mini-batches and used for the training of the NN. This refer this NN as Q-network with parameter, $ \theta $, which minimizes the sequences of loss functions $ L_i (\theta_i) $ :</p>
<p>$ L_i(\theta_i) $ = $ \mathbb{E_{s,a \sim \rho(.)}} [ (y_i - Q(s, a; \theta_i)^2 ] $</p>
<p>Where $ y_i = \mathbb{E_{s' \sim  \varepsilon}} [ r + \gamma \underset{a'} max (s', a', \theta_{i-}) | s,a] $</p>
<p>is the target for iteration i.</p>
<p>They used the previous iteration parameter value ($ \theta_{i-1} $) in order to calculate the target ($y_i$). The parameter ($ \theta_{i-1} $) from previous iteration won't change for some long future iterations, which makes it stationary and training will be smooth. They also  feed concatenation of four video frames as an input to the CNN in order to avoid the partial observation contraints in the learning. Using four frames, CNN will be able locate the movement direction, speed of the objects in the frames.</p>
<p>DQN is used to train on Atari 2600 games. The video frames from emulator are the observations based on discrete actions (up, down, left, rigth..) of the agent in the environment. The network consists of two convolutional layers and two fully connected layers. The last layer outputs the distribution over possible actions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2020-11-08T19:25:42+05:45">
            2020-11-08
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/paper-summary-policy-gradient-methods-for-rl-with-function-approximation/">Paper Summary : Policy Gradient Methods for Reinforcement Learning with Function Approximation</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf">"Policy Gradient Methods for Reinforcement Learning with Function Approximation"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="../posts/paper-summary-policy-gradient-methods-for-rl-with-function-approximation/#Motivation">¶</a>
</h5>
<p>Reinforcement Learning (RL) solves the problem of learning through experiments in the (dynamic) environments. The learner objective is to find an optimal policy which can guide the agent for the nagivation. This optimal policy is formulated in terms of maximizing future reward of the agent. Value-function $ V_{\pi} (s) $ and action-value function $ Q_{\pi}(s,a) $ are the measure of potential future rewards.</p>
<ul>
<li>$ V_{\pi} (s) $ : Goodness measure to be in a state s and then following policy $ \pi $</li>
<li>$ Q_{\pi}(s,a) $ : Goodness measure to be in a state s, perform action a and then follow policy $ \pi $</li>
</ul>
<p>NOTE</p>
<ul>
<li>Both $ V_{\pi} (s) $ and $ Q_{\pi}(s,a) $ are related to rewards in terms of expectation of the discounted future reards and their values are maintained on a lookup table.</li>
<li>Goal : We want to find the value of (state) or (state,action) in a given environment, so that the agent can follow an optimal path, collecting maximum rewards.</li>
</ul>
<p>In a large scale RL problem, maintaining lookup table will lead to the the problem of 'curse of dimensionality'. Currently, this problem is solved using function approximation. The function approximation tries to generalize the estimation of value of state or state-action value based on a set of features in a given state/observations. Most of the existing approaches follow the idea of approximating the value function and then deriving policy out of it. Authors have pointed out two major limitations of this approach:</p>
<p>a. This approach focused towards finding deterministic policy, which might not be the case for complex problems/environments.
b. Small variation in the value estimation might cause different action selection; derived policy is sensitive.</p>
<h5 id="Contribution">Contribution<a class="anchor-link" href="../posts/paper-summary-policy-gradient-methods-for-rl-with-function-approximation/#Contribution">¶</a>
</h5>
<p>Authors proposed an alternative way to approximate policy directly using parameterized function. So, we won't be storing any Q-values in a table, but, learnt using a function approximator. For an example, the policy can be represented by a Neural Network (NN) where we can feed state as input and get probability distribution for action selection as output. Considering $ \theta $ as parameters of the NN, representing the policy and $ \rho $ as its performance measure (which can be a loss function), then the parameter $ \theta $ will be updated as:</p>
<p>$ \theta_{t+1} \gets \theta_t + \alpha \frac{\partial {\rho}}{ \partial{\theta}} $</p>
<h6 id="Policy-Gradient-Theorem:">Policy Gradient Theorem:<a class="anchor-link" href="../posts/paper-summary-policy-gradient-methods-for-rl-with-function-approximation/#Policy-Gradient-Theorem:">¶</a>
</h6>
<p>For any Markov Decision Process (MDP),</p>
<p>$ \nabla_{\theta} J(\theta) = \frac{\partial {\rho(\pi)}}{ \partial{\theta}} = \underset{s} \sum d^{\pi} (s)  \underset{a} \sum \frac{\partial{\pi(s,a)}}{\partial(\theta)} Q^{\pi}(s,a) $ ----------(a)</p>
<p>Here $ \rho(\pi) $ , the average rewards under current policy ($ \pi $) and $ d^{\pi}(s) $, stationary distribution of states under $ \pi $</p>
<ul>
<li>The problem with the above formulation is 'how to get Q(s,a) ?' -&gt; Q(s,a) must be estimated.</li>
</ul>
<p>We can see that the state distribution is independent of policy parameter $ \theta $. Since, gradient is independent of MDP dynamics, it allows model-free learning in RL. If we estimate the policy gradient using Monte-Carlo sampling, it will give REINFORCE algorithm.</p>
<p>In Monte-Carlo sampling, we take N trajectories using current policy $ \pi $ and collect the returns. However, these returns hae high variance and we might need many episodes for the smooth convergence. The variance is introduced due to the fact that we won't be able to collect same trajectories multiple times(.i.e movement of agent is also dynamic) using out stochastic policies in the stochastic environment.</p>
<ul>
<li>QUESTION : How to estimate Q-value in equation (a) ?</li>
</ul>
<p>Authors used a function approximation $ f_w (s,a) $ with parameters 'w' to estimate $ Q^{\pi} (s,a) $ as :</p>
<p>$ \nabla_{\theta} J(\theta) =  \underset{s} \sum d^{\pi(s)} (s)  \underset{a} \sum \frac{\partial{\pi(s,a)}}{\partial(\theta)} f_w(s,a) $ --------- (b)</p>
<p>Here $ f_w(s,a) $ is learnt by following $ \pi $ and updating 'w' by minimizing mean-square error between Q-values $ [ Q^{\pi}(s,a) - f_w(s,a) ]^2 $. The neural network/policy will predict some Q-value and also when agent take some action in the environment, we predict Q-value for a given state/action. Algorithm will try to make sure difference between these two remains as close as possible.</p>
<p>The resulting formulation (b) gives the idea of actor-critic architecture for RL where</p>
<p>i. $ \pi(s,a) $ is the actor which is learning to approximate the policy by maximixing (b)</p>
<p>ii. The critic $ f_w(s,a) $ learning to estimate the policy by minimizing MSE with estimated and true Q-values.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2020-11-01T02:03:54+05:45">
            2020-11-01
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/paper-summary-proximal-policy-optimization-algorithms/">Paper Summary : Proximal Policy Optimization Algorithms</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://arxiv.org/abs/1707.06347">"Proximal Policy Optimization Algorithms"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="../posts/paper-summary-proximal-policy-optimization-algorithms/#Motivation">¶</a>
</h5>
<p>Deep Q learning, 'Vanilla' Policy Gradient, REINFORCE are the examples of approaches for function approximation in RL. When it comes to RL, robustness and sample efficiency are the measures that defines effectivenss of the applied algorithm.</p>
<p>In RL formulation, the agent needs to solve a task/problem in an envronment. Agent countinously interacts with the environment, which provides rewards to the agent, and thereafter agent learns a policy to navigate and tackle the problem. In every time step, RL agent has to make a decision by selecting preferrable action. To do so, agent fully relies on the information of current state and accumulated knowledge (history of rewards) up to current time step. Once the action is performed, the next state/observation is defined by some (stochastic) transition probability model. Also, reward will be signaled to the agent based on this new state information and performed action to get there. In overall, the goal of the agent is to maximize expected cumulative rewards.</p>
<p>In terms of RL, this goal can be formulated as finding a policy $ \pi (a_t | s_t) $ such that expected reward $ \mathbb{E_{\pi_{\theta, \tau}}} [G_t] $ is maximized.</p>
<p>In high dimensional/ countinous action space, policy gradient method can be used to solve this problem. In "vanilla" policy gradient method, the policy is parameterized by some parameter $ \theta $ .i.e parametric policy  $ \pi_{\theta} (a_t | s_t) $ and we directly optimize the policy by finding the optimal parameter $ \theta $.</p>
<p>Even though 'Vanilla' policy gradient/ REINFORCE are simple/easier to implement, they come with some learning issues:</p>
<p>PROBLEM : Usually give rise to high variance while estimating gradient. This is because, the objective function</p>
<p>$ J(\theta) =  \mathbb{E_{\pi_{\theta, \tau}}} (G_t) $</p>
<p>contains expectation; so we can't directly compute the exact gradient. We use stochatic gradient estimates such as REINFORCE based on some batch of trajectories. This sampling approximation adds some variance. That means, we need large number of trajectories to get the best estimation.
In addition, we can see that collecting trajectories could be a problem in complex environments-might take long time to run.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="../posts/paper-summary-proximal-policy-optimization-algorithms/#Contribution">¶</a>
</h5>
<p>Authors have introduced a family of policy optimization methods which is build up on the basis of work of Trust-Region Policy optimization. Two main ideas:</p>
<p>a. Clipped Surrogate Objective Function : It avoids large deviations of learned policy $ \pi_{\theta} $ from old policy $ \pi_{\theta old} $; which is formulated as:</p>
<p>$ L^{clip}(\theta) = \mathbb{E}_t [ min (r_t (\theta) \hat{A_t}, clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A_t}) ] $</p>
<p>Here $ r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta old}(a_t | s_t)} $</p>
<p>$ \epsilon $ is hyperparameter, which restricts the new policy from being too far from old policy.
$ \hat{A_t} $ can be discounted return or advantage function.</p>
<p>The clipping ensures the updates take place in "trust-region". Also, introduces less variance than vanilla gradient methods</p>
<p>b. Multiple Epochs for policy update:</p>
<p>PPO allows to run multiple epochs on the same trajectories and optimize the objective $ L^{clip}(\theta) $. This also reduces the sample inefficieny while learning. In order to collect data, PPO runs policy with parallel actors and then samples mini-batches of the data for training k-epochs using the objective function above.</p>
<p>Let's observe the behaviour of the objective function based on changes in advantage function.</p>
<p>CASE I : When $ \hat{A_t} $ is +ve :</p>
<p>The objective function can be written as :</p>
<p>$ L^{clip} (\theta) = min ( r_t(\theta), 1 + \epsilon ) \hat{A_t} $</p>
<p>Since $\hat{A_t} $ is +ve, when the action occurrence likelihood increases (i.e. $ \pi_{\theta}(a_t | s_t) $), the whole objective value will also increase.
The min operator limits the increasing objective value. So, when  $ \pi_{\theta}(a_t | s_t) ) &gt; (1 + \epsilon) \pi_{\theta old}(a_t | s_t)  $, ceiling occurs at $ (1 + \epsilon) \hat{A_t} $</p>
<p>CASE II : When $ \hat{A_t} $ is -ve :</p>
<p>The objective function can be written as :</p>
<p>$ L^{clip} (\theta) = max ( r_t(\theta), 1 - \epsilon ) \hat{A_t} $</p>
<p>Now, the objective function value only increases when the likelihood of the action is less likely (i.e. $ if \pi_{\theta}(a_t | s_t) $ decreases, $ L^{clip} (\theta) $ increases )
In this case, when $  (1 - \epsilon) \pi_{\theta old}(a_t | s_t) &gt; \pi_{\theta}(a_t | s_t) )   $, max operator limits the value at $ (1 - \epsilon) \pi_{\theta old}(a_t | s_t) \hat{A_t} $</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2020-10-31T22:22:42+05:45">
            2020-10-31
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/paper-summary-learning-what-data-to-learn/">Paper Summary : Learning What Data to Learn</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://arxiv.org/pdf/1702.08635.pdf">"Learning What Data to Learn"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="../posts/paper-summary-learning-what-data-to-learn/#Motivation">¶</a>
</h5>
<p>The performance of learning algorithms based on Machine Learning or Deep Learning rely on amount of training data. Having more data points also has benefit of learning more generalized models and avoiding overfitting. However, collecting data is a painstalking work. Instead, we can learn automatic and adaptive data selection in the training process and make learning faster with minimal data points.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="../posts/paper-summary-learning-what-data-to-learn/#Contribution">¶</a>
</h5>
<p>In this paper, authors have introduced  Neural Data Filter (NDF) as an adaptive framework which can learn data selection policy using deep reinforcement learning(DRL) algorithm 'Policy Gradient'.
Two important aspects of this framework:</p>
<p>a. NDF filter the data instances from randomly fetched mini-batches of data during training process.
b. Training loop provides feedback to NDF policy based on reward signal (e.g. calculated in validation set) and NDF policy is trained using DRL.</p>
<h6 id="NDF-in-detail">NDF in detail<a class="anchor-link" href="../posts/paper-summary-learning-what-data-to-learn/#NDF-in-detail">¶</a>
</h6>
<p>NDF is designed to filter out some portion of training data based on some quality measure. The filtered high-quality data points speed up the convergence of the model.</p>
<p>In order to formulate Markov Decision Process (MDP) in NDF, authors used 'SGD-MDP' with following tuple:
&lt;s, a, P, r, $ \gamma $&gt;</p>
<ul>
<li>s : representing mini-batch data and current state of training model (weights/biases) as a state</li>
<li>a : binary filtering actions;  $ a = {\{a_m\}}_{m=1}^M \in (0, 1)^M $, M-batch size and $ a_m \in \{0,1\} $
indicating whether a particular data instance in minibatch will be selected or not.</li>
<li>P : P(s`| s, a) is a transition probability</li>
<li>r = r(s,a), reward signal based on performance of the current model under consideration (e.g. validation accuracy),</li>
<li>$ \gamma \in [0,1] $, discounting factor</li>
</ul>
<p>The  NDF policy A(s,a, $ \Theta $) can be represented by a binary classification algorithm such as logistic regression or deep NN, where $ \Theta $ is policy parameter and it is updated as:</p>
<p>$ \Theta \gets \Theta + \alpha V_t \sum_m \frac{\partial log P_{\Theta} (a_m|s_m)}{\partial \Theta} $</p>
<p>and, $ V_t $ is the sampled estimation of reward $ R(s_t, a_t) $ from one episode.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2020-10-31T21:18:59+05:45">
            2020-10-31
        </time></footer></article><article class="post post"><header class="post-header"><h2 class="post-title"><a href="../posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/">Paper Summary : Curiosity-driven Exploration by Self-supervised Prediction</a></h2>
    </header><section class="post-excerpt"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://arxiv.org/abs/1705.05363">"Curiosity-driven Exploration by Self-supervised Prediction"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="../posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/#Motivation">¶</a>
</h5>
<p>The policy learning process in Reinforcement Learning (RL) is usually suffered due to delayed/sparse rewards. Reward is a direct signal for an agent to evaluate 'how good the current action selection is'. Since reward collection takes time, learning optimal policy also takes longer time to derive. Another factor that influence the learning process is human-designed reward function. These reward functions might not represent the optimal guidance for learning of the agent or won't be scalable to real world problems. We need a way to overcome reward sparsity and also improves exploration of the agent to make learning more robust.</p>
<p>Human learning process is not only guided by the final goal or achievement, but also driven by motivation or curiosity of the being. Curiosity adds exploratory behaviour to the agent, allowing it to acquire new skills and gain new knowledge about the environment. It also makes agent robust to perform actions which ultimately reduces uncertaintly on it's behaviours to capture the consequences of it's own action.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="../posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/#Contribution">¶</a>
</h5>
<p>The authors, in this paper, proposed curiosity-driven learning by uing agent-intrinsic reward (.i.e a reward which is learnt by agent itself by understanding the current environment or possible changes in the states while navigation). In order to quantify curiosity, they have introduced "Intrinsic Curiosity Module".</p>
<h6 id="Intrinsic-Curiosity-Module-(ICM)">Intrinsic Curiosity Module (ICM)<a class="anchor-link" href="../posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/#Intrinsic-Curiosity-Module-(ICM)">¶</a>
</h6>
<p>The output of ICM is the state prediction error, which serves as reward for curiosity. This module has two sub-components, each represented by neural networks.</p>
<p>a. Inverse Model :</p>
<p>This model learns feature space using self-supervision. This new feature space is learnt in order to avoid features/information which are irrelevant to the agent while nagivation. Learning feature space is completed within two sub-modules:</p>
<p>i) First module encodes the raw input state ($s_t$) into a feature vector ($ \phi(s_t) $)
ii) Second module takes $ \phi(s_t) $ and $ \phi(s_{t+1}) $) as encoded feature inputs and predicts action $ \hat{a_t} $ that agent might take to go to $ s_{t+1} $ from $ s_t $</p>
<p>$ \hat{a_t} = g(  \phi(s_t), \phi(s_{t+1}), \theta_i ) $</p>
<p>Here function g represents NN and  $ \hat{a_t} $ is estimated action. The learnable parameters $ \theta_i $ are trained with loss function representing difference between predicted action and actual action. i.e $ L_I( \hat{a_t}, a_t) $</p>
<p>b. Forward Model :</p>
<p>This is a NN which predicts the next state ($ s_{t+1} $) with inputs $  \phi(s_t) $ and action executed at $ s_t $.</p>
<p>$ \hat{\phi(s_{t+1})} = f( \phi(s_t), a_t, \theta_F) $</p>
<p>$ \hat{\phi(s_{t+1})}$ is the predicted estimation of $ \phi(s_{t+1})$ and $ \theta_F $ represents trainable parameters, with loss function as:</p>
<p>$ L_F ( \phi(s_{t+1}),  \hat{\phi(s_{t+1})}) = \frac{1}{2} ||  \hat{\phi(s_{t+1})} - \phi(s_{t+1})  ||^2 = \eta L_F $</p>
<p>Both losses can be jointly expressed as :</p>
<p>$ \underset{\theta_i, \theta_F} {max} [ (1-\beta) L_I  + \beta L_F ] $</p>
<p>NOTE:</p>
<p>** ICM worked with two connected modules - inverse model (which learnt the feature representation of state and next state) and forward model ( which predicts the feature representation of the next state)
** Curiosity can be calculated by the difference between output of forward model i.e $ \hat{\phi(s_{t+1})} $ and output of the inverse model $  \phi(s_{t+1}) $.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-meta">
                Sijan Bhandari

        on

        <time class="post-date" datetime="2020-10-31T20:25:29+05:45">
            2020-10-31
        </time></footer></article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-3.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blog-sijanb-com-np";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></main><footer class="site-footer clearfix"><section class="poweredby">Contents © 2024         <a href="mailto:sijanonly@gmail.com">Sijan Bhandari</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </section></footer>
</div>

    <script type="text/javascript" src="../assets/js/jquery.js"></script><script type="text/javascript" src="../assets/js/jquery.fitvids.js"></script><script type="text/javascript" src="../assets/js/index.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116715433-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116715433-2');
</script>
</body>
</html>
