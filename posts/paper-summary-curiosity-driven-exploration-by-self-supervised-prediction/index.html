<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Paper Summary : Curiosity-driven Exploration by Self-supervised Prediction | CODEBUG</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../../assets/css/screen.css">
<link rel="stylesheet" type="text/css" href="../../assets/css/nav.css">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://sijanb.com.np/posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/">
<link rel="icon" href="../../favicon.ico" sizes="16x16">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<meta name="author" content="Sijan Bhandari">
<link rel="prev" href="../paper-summary-asynchronous-methods-for-deep-reinforcement-learning/" title="Paper Summary : Asynchronous Methods for Deep Reinforcement Learning" type="text/html">
<link rel="next" href="../paper-summary-learning-what-data-to-learn/" title="Paper Summary : Learning What Data to Learn" type="text/html">
<meta property="og:site_name" content="CODEBUG">
<meta property="og:title" content="Paper Summary : Curiosity-driven Exploration by Self-supervised Predic">
<meta property="og:url" content="https://sijanb.com.np/posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/">
<meta property="og:description" content='Summary of the paper "Curiosity-driven Exploration by Self-supervised Prediction"






Motivation¶The policy learning process in Reinforcement Learning (RL) is usually suffered due to delayed/sparse '>
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-10-31T20:25:29+05:45">
</head>
<body class="nav-closed">

<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
<li class="nav-opened" role="presentation">
            <a href="../../blog/">Blog</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../../categories/">Tags</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../../rss.xml">RSS feed</a>
        </li>
    
    
    </ul>
</div>
<span class="nav-cover"></span>

<div class="site-wrapper">
    <header class="main-header post-head no-cover"><nav class="main-nav overlay clearfix"><a class="blog-logo" href="https://sijanb.com.np/"><img src="../../images/logo.png" alt="Blog Logo"></a>
            <a class="menu-button" href="#"><span class="burger">☰</span><span class="word">Menu</span></a>
        </nav></header><main id="content" class="content" role="main"><article class="post post"><header class="post-header"><h1 class="post-title">Paper Summary : Curiosity-driven Exploration by Self-supervised Prediction</h1>
        <section class="post-meta"> by
            Sijan Bhandari
            on
            <time class="post-date" datetime="2020-10-31T20:25:29+05:45">
                2020-10-31
            </time></section></header><section class="post-content"><div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://arxiv.org/abs/1705.05363">"Curiosity-driven Exploration by Self-supervised Prediction"</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="#Motivation">¶</a>
</h5>
<p>The policy learning process in Reinforcement Learning (RL) is usually suffered due to delayed/sparse rewards. Reward is a direct signal for an agent to evaluate 'how good the current action selection is'. Since reward collection takes time, learning optimal policy also takes longer time to derive. Another factor that influence the learning process is human-designed reward function. These reward functions might not represent the optimal guidance for learning of the agent or won't be scalable to real world problems. We need a way to overcome reward sparsity and also improves exploration of the agent to make learning more robust.</p>
<p>Human learning process is not only guided by the final goal or achievement, but also driven by motivation or curiosity of the being. Curiosity adds exploratory behaviour to the agent, allowing it to acquire new skills and gain new knowledge about the environment. It also makes agent robust to perform actions which ultimately reduces uncertaintly on it's behaviours to capture the consequences of it's own action.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="#Contribution">¶</a>
</h5>
<p>The authors, in this paper, proposed curiosity-driven learning by uing agent-intrinsic reward (.i.e a reward which is learnt by agent itself by understanding the current environment or possible changes in the states while navigation). In order to quantify curiosity, they have introduced "Intrinsic Curiosity Module".</p>
<h6 id="Intrinsic-Curiosity-Module-(ICM)">Intrinsic Curiosity Module (ICM)<a class="anchor-link" href="#Intrinsic-Curiosity-Module-(ICM)">¶</a>
</h6>
<p>The output of ICM is the state prediction error, which serves as reward for curiosity. This module has two sub-components, each represented by neural networks.</p>
<p>a. Inverse Model :</p>
<p>This model learns feature space using self-supervision. This new feature space is learnt in order to avoid features/information which are irrelevant to the agent while nagivation. Learning feature space is completed within two sub-modules:</p>
<p>i) First module encodes the raw input state ($s_t$) into a feature vector ($ \phi(s_t) $)
ii) Second module takes $ \phi(s_t) $ and $ \phi(s_{t+1}) $) as encoded feature inputs and predicts action $ \hat{a_t} $ that agent might take to go to $ s_{t+1} $ from $ s_t $</p>
<p>$ \hat{a_t} = g(  \phi(s_t), \phi(s_{t+1}), \theta_i ) $</p>
<p>Here function g represents NN and  $ \hat{a_t} $ is estimated action. The learnable parameters $ \theta_i $ are trained with loss function representing difference between predicted action and actual action. i.e $ L_I( \hat{a_t}, a_t) $</p>
<p>b. Forward Model :</p>
<p>This is a NN which predicts the next state ($ s_{t+1} $) with inputs $  \phi(s_t) $ and action executed at $ s_t $.</p>
<p>$ \hat{\phi(s_{t+1})} = f( \phi(s_t), a_t, \theta_F) $</p>
<p>$ \hat{\phi(s_{t+1})}$ is the predicted estimation of $ \phi(s_{t+1})$ and $ \theta_F $ represents trainable parameters, with loss function as:</p>
<p>$ L_F ( \phi(s_{t+1}),  \hat{\phi(s_{t+1})}) = \frac{1}{2} ||  \hat{\phi(s_{t+1})} - \phi(s_{t+1})  ||^2 = \eta L_F $</p>
<p>Both losses can be jointly expressed as :</p>
<p>$ \underset{\theta_i, \theta_F} {max} [ (1-\beta) L_I  + \beta L_F ] $</p>
<p>NOTE:</p>
<p>** ICM worked with two connected modules - inverse model (which learnt the feature representation of state and next state) and forward model ( which predicts the feature representation of the next state)
** Curiosity can be calculated by the difference between output of forward model i.e $ \hat{\phi(s_{t+1})} $ and output of the inverse model $  \phi(s_{t+1}) $.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
    </section><footer class="post-footer"><section class="comments hidden-print"><h2>Comments</h2>
        
    
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="blog-sijanb-com-np",
            disqus_url="https://sijanb.com.np/posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction/",
        disqus_title="Paper Summary : Curiosity-driven Exploration by Self-supervised Prediction",
        disqus_identifier="cache/posts/paper-summary-curiosity-driven-exploration-by-self-supervised-prediction.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></footer></article><script>var disqus_shortname="blog-sijanb-com-np";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script></main><footer class="site-footer clearfix"><section class="poweredby">Contents © 2024         <a href="mailto:sijanonly@gmail.com">Sijan Bhandari</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </section></footer>
</div>

    <script type="text/javascript" src="../../assets/js/jquery.js"></script><script type="text/javascript" src="../../assets/js/jquery.fitvids.js"></script><script type="text/javascript" src="../../assets/js/index.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116715433-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116715433-2');
</script>
</body>
</html>
