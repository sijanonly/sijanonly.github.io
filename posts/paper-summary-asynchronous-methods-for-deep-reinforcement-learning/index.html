<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Paper Summary : Asynchronous Methods for Deep Reinforcement Learning | CODEBUG</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/custom.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../../assets/css/screen.css">
<link rel="stylesheet" type="text/css" href="../../assets/css/nav.css">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400%7CInconsolata">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://sijanb.com.np/posts/paper-summary-asynchronous-methods-for-deep-reinforcement-learning/">
<link rel="icon" href="../../favicon.ico" sizes="16x16">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Sijan Bhandari">
<link rel="prev" href="../paper-summary-visual-reinforcement-learning-imagined-goals/" title="Paper Summary : Visual Reinforcement Learning Imagined Goals" type="text/html">
<meta property="og:site_name" content="CODEBUG">
<meta property="og:title" content="Paper Summary : Asynchronous Methods for Deep Reinforcement Learning">
<meta property="og:url" content="https://sijanb.com.np/posts/paper-summary-asynchronous-methods-for-deep-reinforcement-learning/">
<meta property="og:description" content='Summary of the paper "Asynchronous Methods for Deep Reinforcement Learning"







Motivation¶Deep Neural Network (DNN) is introduced to Reinforcement Learning (RL) framework in order to make function'>
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-10-31T17:26:55+05:45">
</head>
<body class="nav-closed">

<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
<li class="nav-opened" role="presentation">
            <a href="../../blog/">Blog</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../../categories/">Tags</a>
        </li>
        <li class="nav-opened" role="presentation">
            <a href="../../rss.xml">RSS feed</a>
        </li>
    
    
    </ul>
</div>
<span class="nav-cover"></span>

<div class="site-wrapper">
    <header class="main-header post-head no-cover"><nav class="main-nav overlay clearfix"><a class="blog-logo" href="https://sijanb.com.np/"><img src="../../images/logo.png" alt="Blog Logo"></a>
            <a class="menu-button" href="#"><span class="burger">☰</span><span class="word">Menu</span></a>
        </nav></header><main id="content" class="content" role="main"><article class="post post"><header class="post-header"><h1 class="post-title">Paper Summary : Asynchronous Methods for Deep Reinforcement Learning</h1>
        <section class="post-meta"> by
            Sijan Bhandari
            on
            <time class="post-date" datetime="2020-10-31T17:26:55+05:45">
                2020-10-31 17:26
            </time></section></header><section class="post-content"><div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Summary of the paper <a href="https://arxiv.org/abs/1602.01783">"Asynchronous Methods for Deep Reinforcement Learning"</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Motivation">Motivation<a class="anchor-link" href="#Motivation">¶</a>
</h5>
<p>Deep Neural Network (DNN) is introduced to Reinforcement Learning (RL) framework in order to make function approximation easier/scable for large state-space problems. DNN itself suffers from overfitting because of the correlated data while nagivating through the environments (e.g. when we play a game, each consecutive moves of a player withing smaller timeframes looks similar, which won't contribute much for learning). In order to avoid it, people started using experience replay, where we have to store navigation experience (e.g. screenshots in games) as a buffer and we can use them later while training/updating policy/model parameters.</p>
<p>This works well, but only for off-policy algorithms like Q-learning. How to use on-policy algorithms like SARSA and make it stable learning using DNN ? Also, using experience replay introduces extra memory requirements/ computatonal delay for each update and real interaction with environment.</p>
<p>**NOTE :</p>
<ul>
<li>On-policy : The training data is generated by the same policy being trained. e.g : Reinforce</li>
<li>Off-Policy : The training data generated from another policy can be used to train the current policy. e.g : Q-learning</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Contribution">Contribution<a class="anchor-link" href="#Contribution">¶</a>
</h5>
<p>In this paper, authors introduced an asynchronous training process by executing multiple agents in parallel in different instances of the same environment using multiple CPU cores. It uses multithreading to run those agents and update the global model parameters asynchronously in online fashion. It is reported that this approach enables stable learning and faster convergence speed.</p>
<p>They have introduced asynchronous variants of SARSA, 1-step/n-step Q learning and advantage actor-critic algorithm. Let's discuss some details on Asynchronous Q-learning and Async. Advantage Actor critic (A3C) algorithms.</p>
<h6 id="Asynchronous-Q-Learning">Asynchronous Q-Learning<a class="anchor-link" href="#Asynchronous-Q-Learning">¶</a>
</h6>
<p>In Deep Q-Learning (DQN), the neural network (NN) approximates the Q-value function Q(s,a, $\theta$) with loss formulated as:
$
\begin{equation}
 L_{i} (\theta_i)  = \mathbb{E} [ r + \gamma Q (s^`, a^`, \theta_{i-1}) - Q(s, a, \theta_i) ]^2 ..........(i)
 \end{equation}
 $</p>
<p>In Async 1-step Q learning, each thread maintains it's own copy of environment and agent traverse through the environment with the help of $  \epsilon $ - greedy policy. At each step, we compute teh gradient of the loss (i) and collect gradients over multiple timesteps before updating the parameters.</p>
<h6 id="Asynchronous-Advantage-Actor-Critic-(A3C)">Asynchronous Advantage Actor-Critic (A3C)<a class="anchor-link" href="#Asynchronous-Advantage-Actor-Critic-(A3C)">¶</a>
</h6>
<p>Actor-critic method combines both value-pased and policy based methods. In Async. setup, it uses a 'forward-view' and n-step updates for both policy and value function.</p>
<p>It has a policy $ \pi(a_t | s_t; \theta) $ and value function $ V (s_t; \theta_t)$ to be learnt. It uses "forward-view", i.e. selecting actions based on its exploration strategy $ \pi (a_t | s_t; \theta) $ up to some $  t_{max} $  steps in the future, to collect up to $ t_{max} $ rewards since last update.</p>
<p>Now, policy and value functions are updated after every $ t_max $ actions as:</p>
<p>*Policy Network : $ \bigtriangledown_{\theta} log \pi(a_t | s_t; \theta) (R_t - V(s_t, \theta_v)) $</p>
<ul>
<li>Value Network : $ \bigtriangledown_{v} (R - V(s_t) $</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
</div>
    </section><footer class="post-footer"><section class="comments hidden-print"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="blog-sijanb-com-np",
            disqus_url="https://sijanb.com.np/posts/paper-summary-asynchronous-methods-for-deep-reinforcement-learning/",
        disqus_title="Paper Summary : Asynchronous Methods for Deep Reinforcement Learning",
        disqus_identifier="cache/posts/paper-summary-asynchronous-methods-for-deep-reinforcement-learning.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></footer></article><script>var disqus_shortname="blog-sijanb-com-np";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script></main><footer class="site-footer clearfix"><section class="poweredby">Contents © 2020         <a href="mailto:sijanonly@gmail.com">Sijan Bhandari</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </section></footer>
</div>

    <script type="text/javascript" src="../../assets/js/jquery.js"></script><script type="text/javascript" src="../../assets/js/jquery.fitvids.js"></script><script type="text/javascript" src="../../assets/js/index.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116715433-2"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116715433-2');
</script>
</body>
</html>
