{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will see how we can train support vector machines using stochastic gradient descent (SGD). Before jumping to the algorithm, we need to know why subgradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of the 0-1 loss and hinge loss:\n",
    "\n",
    "<img src='/images/svm_ill.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we don't we use 0-1 loss ?? The obvious reason is it is not convex. Another factor could be it's reaction to small changes in parameters. You can see from the graph, if you change (w,b) the loss will flip to either 0 or 1 very fast without acknowledging the in between values.\n",
    "The hinge loss,on the other hand, has smooth change until it reaches '1' along x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='/images/svm_obj_1.jpg'>\n",
    "\n",
    "<img src='/images/svm_obj_2.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_regularization(w, subgradient_w):\n",
    "    \"\"\"\n",
    "    The total loss :( 1/2 * ||w||^2 + Hingle_loss) has w term to be added after getting subgradient of 'w'\n",
    "    \n",
    "      total_w = regularization_term + subgradient_term\n",
    "    i.e total_w = w + C *  âˆ‘ (-y*x)\n",
    "    \n",
    "    \"\"\"\n",
    "    return w + subgradient_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradients(x, y, w, b, C):\n",
    "    \"\"\"\n",
    "    :x: inputs [[x1,x2], [x2,x2],...]\n",
    "    :y: labels [1, -1,...]\n",
    "    :w: initial w\n",
    "    :b: initial b\n",
    "    :C: tradeoff/ hyperparameter\n",
    "    \n",
    "    \"\"\"\n",
    "    subgrad_w = 0\n",
    "    subgrad_b = 0\n",
    "    \n",
    "    # sum over all subgradients of hinge loss for a given samples x,y\n",
    "    for x_i, y_i in zip(x,y):\n",
    "        f_xi = np.dot(w.T, x_i) + b\n",
    "\n",
    "        decision_value = y_i * f_xi\n",
    "\n",
    "        if decision_value < 1:\n",
    "            subgrad_w += - y_i*x_i\n",
    "            subgrad_b += -1 * y_i\n",
    "        else:\n",
    "            subgrad_w += 0\n",
    "            subgrad_b += 0\n",
    "    \n",
    "    # multiply by C after summation of all subgradients for a given samples of x,y\n",
    "    subgrad_w = C * subgrad_w\n",
    "    subgrad_b = C * subgrad_b\n",
    "    return (add_regularization(w, subgrad_w), subgrad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgrad_descent(data, initial_values, B, C, T=1):\n",
    "    \"\"\"\n",
    "    :data: Pandas data frame\n",
    "    :initial_values: initialization for w and b\n",
    "    :B: sample size for random data selection\n",
    "    :C: hyperparameter, tradeoff between hard margin and hinge loss\n",
    "    :T: # of iterations\n",
    "    \n",
    "    \"\"\"\n",
    "    w, b = initial_values\n",
    "    for t in range(1, T+1):\n",
    "        \n",
    "        # randomly select B data points \n",
    "        training_sample = data.sample(B)\n",
    "        \n",
    "        # set learning rate\n",
    "        learning_rate = 1/t\n",
    "        \n",
    "        # prepare inputs in the form [[h1, w1], [h2, w2], ....]\n",
    "        x = training_sample[['height', 'weight']].values\n",
    "      \n",
    "        # prepare labels in the form [1, -1, 1, 1, - 1 ......]\n",
    "        y = training_sample['gender'].values\n",
    "      \n",
    "        sub_grads = subgradients(x,y, w, b, C)\n",
    "        \n",
    "        # update weights\n",
    "        w = w - learning_rate * sub_grads[0]\n",
    "        \n",
    "        # update bias\n",
    "        b = b - learning_rate * sub_grads[1]\n",
    "    return (w,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>183</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184</td>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "      <td>53</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   height  weight  gender\n",
       "0     167      82       1\n",
       "1     183      57      -1\n",
       "2     184      50      -1\n",
       "3     178      53      -1\n",
       "4     166      72       1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data['height'] = np.random.randint(160, 190, 100)\n",
    "data['weight'] = np.random.randint(50, 90, 100)\n",
    "\n",
    "data['gender'] = [-1 if value < 5 else 1 for value in np.random.randint(0,10, 100)]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([-2, -3])\n",
    "initial_bias = 12\n",
    "\n",
    "initial_values = (initial_weights, initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = stochastic_subgrad_descent(data, initial_values, 20, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.798,  4.648]), 14.891692339980313)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "category": "",
   "date": "2019-05-26 23:52:15 UTC+05:45",
   "description": "",
   "link": "",
   "slug": "implementation-of-stochastic-subgradient-descent-for-support-vector-machine-using-python",
   "tags": "",
   "title": "Implementation of stochastic subgradient descent for support vector machine using Python",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
